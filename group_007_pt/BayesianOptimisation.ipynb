{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom torch.nn import init\nimport numpy as np\n\ndef conv3x3(in_channels, out_channels, stride=1, \n            padding=1, bias=True, groups=1):    \n    return nn.Conv2d(\n        in_channels,\n        out_channels,\n        kernel_size=3,\n        stride=stride,\n        padding=padding,\n        bias=bias,\n        groups=groups)\n\ndef upconv2x2(in_channels, out_channels, mode='transpose'):\n    if mode == 'transpose':\n        return nn.ConvTranspose2d(\n            in_channels,\n            out_channels,\n            kernel_size=2,\n            stride=2)\n    else:\n        # out_channels is always going to be the same\n        # as in_channels\n        return nn.Sequential(\n            nn.Upsample(mode='bilinear', scale_factor=2),\n            conv1x1(in_channels, out_channels))\n\ndef conv1x1(in_channels, out_channels, groups=1):\n    return nn.Conv2d(\n        in_channels,\n        out_channels,\n        kernel_size=1,\n        groups=groups,\n        stride=1)\n\n\nclass DownConv(nn.Module):\n    \"\"\"\n    A helper Module that performs 2 convolutions and 1 MaxPool.\n    A ReLU activation follows each convolution.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, pooling=True):\n        super(DownConv, self).__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.pooling = pooling\n\n        self.conv1 = conv3x3(self.in_channels, self.out_channels)\n        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n\n        if self.pooling:\n            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        before_pool = x\n        if self.pooling:\n            x = self.pool(x)\n        return x, before_pool\n\n\nclass UpConv(nn.Module):\n    \"\"\"\n    A helper Module that performs 2 convolutions and 1 UpConvolution.\n    A ReLU activation follows each convolution.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, \n                 merge_mode='concat', up_mode='transpose'):\n        super(UpConv, self).__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.merge_mode = merge_mode\n        self.up_mode = up_mode\n\n        self.upconv = upconv2x2(self.in_channels, self.out_channels, \n            mode=self.up_mode)\n\n        if self.merge_mode == 'concat':\n            self.conv1 = conv3x3(\n                2*self.out_channels, self.out_channels)\n        else:\n            # num of input channels to conv2 is same\n            self.conv1 = conv3x3(self.out_channels, self.out_channels)\n        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n\n    def forward(self, from_down, from_up):\n        \"\"\" Forward pass\n        Arguments:\n            from_down: tensor from the encoder pathway\n            from_up: upconv'd tensor from the decoder pathway\n        \"\"\"\n        from_up = self.upconv(from_up)\n        if self.merge_mode == 'concat':\n            x = torch.cat((from_up, from_down), 1)\n        else:\n            x = from_up + from_down\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        return x\n\n\nclass UNet(nn.Module):\n    \"\"\" `UNet` class is based on https://arxiv.org/abs/1505.04597\n\n    The U-Net is a convolutional encoder-decoder neural network.\n    Contextual spatial information (from the decoding,\n    expansive pathway) about an input tensor is merged with\n    information representing the localization of details\n    (from the encoding, compressive pathway).\n\n    Modifications to the original paper:\n    (1) padding is used in 3x3 convolutions to prevent loss\n        of border pixels\n    (2) merging outputs does not require cropping due to (1)\n    (3) residual connections can be used by specifying\n        UNet(merge_mode='add')\n    (4) if non-parametric upsampling is used in the decoder\n        pathway (specified by upmode='upsample'), then an\n        additional 1x1 2d convolution occurs after upsampling\n        to reduce channel dimensionality by a factor of 2.\n        This channel halving happens with the convolution in\n        the tranpose convolution (specified by upmode='transpose')\n    \"\"\"\n\n    def __init__(self, num_classes, in_channels=3, depth=5, \n                 start_filts=64, up_mode='transpose', \n                 merge_mode='concat'):\n        \"\"\"\n        Arguments:\n            in_channels: int, number of channels in the input tensor.\n                Default is 3 for RGB images.\n            depth: int, number of MaxPools in the U-Net.\n            start_filts: int, number of convolutional filters for the \n                first conv.\n            up_mode: string, type of upconvolution. Choices: 'transpose'\n                for transpose convolution or 'upsample' for nearest neighbour\n                upsampling.\n        \"\"\"\n        super(UNet, self).__init__()\n\n        if up_mode in ('transpose', 'upsample'):\n            self.up_mode = up_mode\n        else:\n            raise ValueError(\"\\\"{}\\\" is not a valid mode for \"\n                             \"upsampling. Only \\\"transpose\\\" and \"\n                             \"\\\"upsample\\\" are allowed.\".format(up_mode))\n    \n        if merge_mode in ('concat', 'add'):\n            self.merge_mode = merge_mode\n        else:\n            raise ValueError(\"\\\"{}\\\" is not a valid mode for\"\n                             \"merging up and down paths. \"\n                             \"Only \\\"concat\\\" and \"\n                             \"\\\"add\\\" are allowed.\".format(up_mode))\n\n        # NOTE: up_mode 'upsample' is incompatible with merge_mode 'add'\n        if self.up_mode == 'upsample' and self.merge_mode == 'add':\n            raise ValueError(\"up_mode \\\"upsample\\\" is incompatible \"\n                             \"with merge_mode \\\"add\\\" at the moment \"\n                             \"because it doesn't make sense to use \"\n                             \"nearest neighbour to reduce \"\n                             \"depth channels (by half).\")\n\n        self.num_classes = num_classes\n        self.in_channels = in_channels\n        self.start_filts = start_filts\n        self.depth = depth\n        self.down_convs = []\n        self.up_convs = []\n\n        # create the encoder pathway and add to a list\n        for i in range(depth):\n            ins = self.in_channels if i == 0 else outs\n            outs = self.start_filts*(2**i)\n            pooling = True if i < depth-1 else False\n\n            down_conv = DownConv(ins, outs, pooling=pooling)\n            self.down_convs.append(down_conv)\n\n        # create the decoder pathway and add to a list\n        # - careful! decoding only requires depth-1 blocks\n        for i in range(depth-1):\n            ins = outs\n            outs = ins // 2\n            up_conv = UpConv(ins, outs, up_mode=up_mode,\n                merge_mode=merge_mode)\n            self.up_convs.append(up_conv)\n\n        self.conv_final = conv1x1(outs, self.num_classes)\n\n        # add the list of modules to current module\n        self.down_convs = nn.ModuleList(self.down_convs)\n        self.up_convs = nn.ModuleList(self.up_convs)\n\n        self.reset_params()\n\n    @staticmethod\n    def weight_init(m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.xavier_normal_(m.weight)\n            nn.init.constant_(m.bias, 0)\n\n\n    def reset_params(self):\n        for i, m in enumerate(self.modules()):\n            self.weight_init(m)\n\n\n    def forward(self, x):\n        encoder_outs = []\n         \n        # encoder pathway, save outputs for merging\n        for i, module in enumerate(self.down_convs):\n            x, before_pool = module(x)\n            encoder_outs.append(before_pool)\n        \n        for i, module in enumerate(self.up_convs):\n            before_pool = encoder_outs[-(i+2)]\n            x = module(before_pool, x)\n        \n        # No softmax is used. This means you need to use\n        # nn.CrossEntropyLoss is your training script,\n        # as this module includes a softmax already.\n        x = self.conv_final(x)\n        return x\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-06T08:52:31.159586Z","iopub.execute_input":"2023-04-06T08:52:31.159906Z","iopub.status.idle":"2023-04-06T08:52:35.603606Z","shell.execute_reply.started":"2023-04-06T08:52:31.159877Z","shell.execute_reply":"2023-04-06T08:52:35.602529Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport os\nfrom PIL import Image\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.datasets.utils as utils\nimport torch.nn.functional as F\n\n#Weigth coef for the Unsupervised\ndef dice_loss(logits, targets, is_modelT_preds=True): \n  if is_modelT_preds:\n    targets_animal = torch.unsqueeze(targets, dim=1)\n  targets_animal = torch.squeeze(targets).to(torch.float32)\n  preds_animal = F.softmax(logits, dim=1)\n  preds_animal = preds_animal[:,1,:,:].to(torch.float32)\n  eps = 1e-6\n  intersection = (preds_animal * targets_animal).sum()\n  dice_coef = (2. * intersection + eps) / ((preds_animal**2).sum() + (targets_animal**2).sum() + eps)\n  dice_loss = 1 - dice_coef\n  return dice_loss\n\n@torch.no_grad()\ndef wt(rampup_length, current, alpha, wait_period = 5):\n\n  if current < wait_period:\n    return 0.0\n    \n  else:\n    if rampup_length == 0:\n                return 1.0\n    else:\n        current -= wait_period\n        current = np.clip(current, 0.0, rampup_length)\n        phase = 1.0 - current / rampup_length\n        return float(alpha * np.exp(-5.0 * phase * phase))\n\n\n#update the Teacher weight\n@torch.no_grad()\ndef update_ema_variables(model, ema_model, alpha, global_step): \n    # Use the true average until the exponential average is more correct\n    alpha = min(1 - 1 / (global_step + 1), alpha)\n    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n\n\n@torch.no_grad()\ndef evaluate_model(model, dataloader):\n  \n  model.eval()\n  intersection_total, union_total = 0, 0\n  pixel_correct, pixel_count = 0, 0\n    \n  for data in dataloader:\n    imgs, labels = data\n    imgs, labels = imgs.cuda(), labels.cuda()\n    logits = model(imgs)\n    preds = torch.argmax(logits, dim=1)\n    targets = torch.squeeze(labels)\n            \n    intersection_total += torch.logical_and(preds, targets).sum()\n    union_total += torch.logical_or(preds, targets).sum()\n            \n    pixel_correct += (preds == targets).sum()\n    pixel_count += targets.numel()\n\n  iou = (intersection_total / union_total).item()\n  accuracy = (pixel_correct / pixel_count).item()\n  \n  return accuracy, iou\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T08:52:35.605622Z","iopub.execute_input":"2023-04-06T08:52:35.606114Z","iopub.status.idle":"2023-04-06T08:52:36.010285Z","shell.execute_reply.started":"2023-04-06T08:52:35.606082Z","shell.execute_reply":"2023-04-06T08:52:36.009151Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport os\nfrom PIL import Image\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.datasets.utils as utils\n\nrandom.seed(200)  # Fix randomness\n\n# mean and std of whole image dataset\nDATA_MEAN = torch.asarray([0.4803, 0.4497, 0.3960])\nDATA_STD = torch.asarray([0.2665, 0.2623, 0.2707])\n\n\n# Each pixel in a mask image can take one of three values: 1, 2, or 3. 1 means that this pixel of an image belongs to the class pet, 2 - to the class background, 3 - to the class border.\ndef preprocess_mask(mask):\n\n    mask[mask == (2.0 / 255)] = 0.0\n    mask[(mask == 1.0 / 255) | (mask == 3.0 / 255)] = 1.0\n    return mask\n\n\nclass OxfordPetDataset_with_labels(Dataset):\n    def __init__(self, images_filenames, images_directory, masks_directory, transform_data_1=None, transform_mask_1=None, transform_2=None):\n        self.images_filenames = images_filenames\n        self.images_directory = images_directory\n        self.masks_directory = masks_directory\n\n        self.transform_data_1 = transform_data_1\n        self.transform_mask_1 = transform_mask_1\n        self.transform_2 = transform_2\n\n    def __len__(self):\n        return len(self.images_filenames)\n\n    def __getitem__(self, idx):\n        image_filename = self.images_filenames[idx]\n        image = Image.open(os.path.join(self.images_directory, image_filename))\n        mask = Image.open(\n            os.path.join(self.masks_directory,\n                         image_filename.replace(\".jpg\", \".png\")),\n        )\n\n        if self.transform_data_1 is not None:\n            image = self.transform_data_1(image)\n\n        if self.transform_mask_1 is not None:\n            mask = self.transform_mask_1(mask)\n\n        mask = preprocess_mask(mask)\n\n        if self.transform_2 is not None:\n            mask = self.transform_2(mask)\n            image = self.transform_2(image)\n\n        mask[mask <= 0.5] = 0\n        mask[mask > 0.5] = 1\n\n        return image, mask\n\ndef readable_images(images_filenames, images_directory):\n    \"\"\"\n    Remove the data that are not readable\n    \"\"\"\n    correct_images_filenames = []\n\n    for i in images_filenames:\n        try:\n            Image.open(os.path.join(images_directory, i))\n            correct_images_filenames.append(i)\n        except:\n            continue\n\n    return correct_images_filenames\n\n\ndef are_images_all_RGB(images_filenames, images_directory):\n    \"\"\"\n    Remove the data that do not have shape of (3,_,_)\n    \"\"\"\n    correct_images_filenames = []\n    transform = transforms.Compose([transforms.ToTensor()])\n    for i in images_filenames:\n        img = Image.open(os.path.join(images_directory, i))\n        img = transform(img)\n        if img.shape[0] == 3:\n            correct_images_filenames.append(i)\n    return correct_images_filenames\n\ndef get_supervised_data(percentage_labelled, percentage_validation, percentage_test, img_resize=64):\n    random.seed(200)\n    images_directory = os.path.join(\"/kaggle/input/oxford-pets/images/images\")\n    masks_directory = os.path.join(\"/kaggle/input/oxford-pets/annotations/annotations/trimaps\")\n    images_filenames = list(sorted(os.listdir(images_directory)))\n    \n    correct_images_filenames = readable_images(\n        images_filenames, images_directory)\n    correct_images_filenames = are_images_all_RGB(\n        correct_images_filenames, images_directory)\n\n    random.shuffle(correct_images_filenames)\n\n    nb_data = len(correct_images_filenames)\n    transform_data_1 = transforms.Compose(\n        [transforms.ToTensor(),\n         transforms.Normalize(DATA_MEAN, DATA_STD)\n         ])\n\n    transform_mask_1 = transforms.Compose(\n        [transforms.ToTensor()])\n\n    transform_2 = transforms.Compose(\n        [transforms.Resize((img_resize, img_resize))]\n    )\n\n    train_images_filenames = correct_images_filenames[0:int(\n        nb_data*percentage_labelled)]\n    validation_images_filenames = correct_images_filenames[int(\n        nb_data*(1-percentage_validation - percentage_test)):int(nb_data*(1 - percentage_test))]\n    test_images_filenames = correct_images_filenames[int(\n        nb_data*(1 - percentage_test)):]\n\n    train_data = OxfordPetDataset_with_labels(train_images_filenames,\n                                              images_directory, masks_directory, transform_data_1, transform_mask_1, transform_2)\n\n    validation_data = OxfordPetDataset_with_labels(\n        validation_images_filenames, images_directory, masks_directory, transform_data_1, transform_mask_1, transform_2)\n\n    # test data\n    test_data = OxfordPetDataset_with_labels(\n        test_images_filenames, images_directory, masks_directory, transform_data_1, transform_mask_1, transform_2)\n    print(f'Loaded {len(images_filenames)} images')\n\n    return train_data, validation_data, test_data\n# (Keep this commented)\n# Example  : mixed_train_loader, val_loader, test_loader = get_data(0.2, 0.8, 0.2, 0.2)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T08:52:36.012910Z","iopub.execute_input":"2023-04-06T08:52:36.013387Z","iopub.status.idle":"2023-04-06T08:52:36.062771Z","shell.execute_reply.started":"2023-04-06T08:52:36.013345Z","shell.execute_reply":"2023-04-06T08:52:36.061703Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"'''\nhttps://pytorch.org/vision/main/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n'''\nimport numpy as np\nfrom torchvision.utils import save_image\nimport torch\nimport torchvision\nfrom random import choice\nfrom torchvision.transforms import RandomInvert, RandomRotation\n\n\n\n\ndef augmentation(batch, masks, var):\n    augmented_batch = batch + torch.empty(batch.shape).normal_(mean=0,std=var)\n    augmented_batch = torch.clip(augmented_batch, 0, 1)\n    inversion = RandomInvert(np.random.lognormal(1,var))\n    augmented_batch = inversion(augmented_batch)\n    augmented_batch = torchvision.transforms.functional.adjust_saturation(augmented_batch, np.random.lognormal(1, var))\n    rotater = RandomRotation(degrees=(-var*90,var*90))\n    augmented_batch = rotater(batch)\n    augmented_mask = rotater(masks)\n    return augmented_batch.type(torch.float32), augmented_mask.type(torch.float32)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T08:52:36.066465Z","iopub.execute_input":"2023-04-06T08:52:36.066884Z","iopub.status.idle":"2023-04-06T08:52:36.074663Z","shell.execute_reply.started":"2023-04-06T08:52:36.066842Z","shell.execute_reply":"2023-04-06T08:52:36.073546Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def train(config):\n    model = UNet(2, in_channels = 3, depth=config['depth'])\n    model = nn.DataParallel(model)\n    model.cuda()\n    losses, accsTr, IousTr, accsVal, IousVal = [], [], [], [], []\n    optimizer = Adam(model.parameters(), lr=config['lr'])\n    loss_fn = dice_loss\n    s = \"/kaggle/working/Models/\" + f\"{config['batch_size']}-{config['gaussian_noise']}-{config['lr']}-{config['lr_decay']}-{config['depth']}/\"\n    if config['lr_decay'] is not None:\n        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=config['lr_decay'])\n    else:\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\n    num_workers = 1\n\n    train_loader = DataLoader(\n        train_data,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers = num_workers\n    )\n    train_loader_eval = DataLoader(\n        train_data,\n        batch_size=128,\n        shuffle=False,\n        num_workers = num_workers\n    )\n    val_loader = DataLoader(\n        val_data,\n        batch_size=128,\n        shuffle=False,\n        num_workers = num_workers\n    )\n\n\n    for epoch in range(max_epochs):\n        model.train()\n        running_loss = 0\n        \n        for step, data in enumerate(train_loader):\n            optimizer.zero_grad()\n            imgs, labels = data\n            \n            imgs_aug, labels_aug = augmentation(imgs, labels, config['gaussian_noise'])\n            imgs_aug, labels_aug = imgs_aug.cuda(), labels_aug.cuda()\n            output = model.forward(imgs_aug)\n\n            loss = loss_fn(output, labels_aug)\n\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        losses.append(running_loss)\n\n        accTr, IouTr = evaluate_model(model, train_loader_eval)  \n        accVal, IouVal = evaluate_model(model, val_loader)   \n\n        accsTr.append(accTr)\n        IousTr.append(IouTr)\n        accsVal.append(accVal)\n        IousVal.append(IouVal)\n        \n        \n        \n        df = pd.DataFrame({'loss': losses, 'accTr': accsTr, 'IouTr': IousTr, 'accVal': accsVal, 'IouVal': IousVal})\n        df.to_csv(s+f\"Results.csv\")\n\n        session.report({'validation_iou': IouVal, 'running_loss': running_loss, 'train_iou': IouTr, 'validation_accuracy': accVal, 'train_accuracy': accTr})\n\n    \ndef main(config):\n    \n    hyperopt_search = HyperOptSearch(\n        metric=\"validation_iou\", mode=\"max\"\n    )\n    hyperopt_search = tune.search.ConcurrencyLimiter(hyperopt_search, max_concurrent=4)\n    resources_per_trial = {\"cpu\": .5, \"gpu\": .5}\n    stopper = tune.stopper.TrialPlateauStopper(\n        metric = 'validation_iou', \n        std = 0.01, \n        num_results = 3, \n        grace_period = 1,\n        mode = 'min',\n        metric_threshold=0.7\n    )\n    tuner = tune.Tuner(\n        tune.with_resources(train, resources=resources_per_trial),\n        tune_config=tune.TuneConfig(\n            search_alg=hyperopt_search,\n            num_samples=50\n        ),\n        param_space=config,\n        run_config=air.RunConfig(stop=stopper)\n    )\n    results = tuner.fit()\n    \n    best_result = results.get_best_result(\"validation_iou\", \"max\")\n    print(\"Best trial config: {}\".format(best_result.config))\n    print(\"Best trial final validation IoU: {}\".format(\n        best_result.metrics[\"validation_iou\"]))\n    print(\"Best trial final validation accuracy: {}\".format(\n        best_result.metrics[\"validation_accuracy\"]))\n    print(\"Best trial final train IoU: {}\".format(\n        best_result.metrics[\"train_iou\"]))\n    print(\"Best trial final train accuracy: {}\".format(\n        best_result.metrics[\"train_accuracy\"]))\n    print(\"Best running loss: {}\".format(\n        best_result.metrics[\"running_loss\"]))\n    \n    return best_result\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T09:44:10.469455Z","iopub.execute_input":"2023-04-06T09:44:10.469892Z","iopub.status.idle":"2023-04-06T09:44:10.494205Z","shell.execute_reply.started":"2023-04-06T09:44:10.469855Z","shell.execute_reply":"2023-04-06T09:44:10.493044Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"!mkdir Models","metadata":{"execution":{"iopub.status.busy":"2023-04-06T08:52:36.101623Z","iopub.execute_input":"2023-04-06T08:52:36.102302Z","iopub.status.idle":"2023-04-06T08:52:37.140997Z","shell.execute_reply.started":"2023-04-06T08:52:36.102264Z","shell.execute_reply":"2023-04-06T08:52:37.139481Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom ray.tune.search.hyperopt import HyperOptSearch\nfrom torch.optim import Adam, lr_scheduler\nimport torch\nimport ray\nfrom ray import tune, air\nfrom ray.air import session\nfrom ray.tune.schedulers import ASHAScheduler\nimport pandas as pd\nimport os\n\n# Do NOT touch these hyperparameters\nimg_resize = 64\npercentage_validation = 0.1\npercentage_test = 0.1\n\n# Change percentage labelled to match the current optimisation parameters.\npercentage_labelled = 0.25\nmax_epochs = 20\n#Training params\nHYPERPARAMETER_SPACE = {\n    'batch_size': tune.choice([16, 32, 64]),\n    'gaussian_noise': tune.choice([0.01, 0.1, 1, 10]),\n    'lr': tune.choice([1e-2, 1e-3, 1e-4, 1e-5]),\n    'lr_decay': tune.choice([0.9, 0.99, 1, None]),\n    'depth': tune.choice([3,4,5]),\n}\n\nfor batch_size in [16, 32, 64]:\n    for gaussian_noise in [0.01, 0.1, 1, 10]:\n        for lr in [1e-2, 1e-3, 1e-4, 1e-5]:\n            for lr_decay in [0.9, 0.99, 1, None]:\n                for depth in [3,4,5]:\n                    os.system(f\"mkdir Models/{batch_size}-{gaussian_noise}-{lr}-{lr_decay}-{depth}\")\nCURRENT_BEST_PARAMS = [{\n    'batch_size': 32,\n    'gaussian_noise': 1,\n    'lr': 1e-3,\n    'lr_decay': 0.99,\n    'depth': 3\n}]\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T08:52:37.143868Z","iopub.execute_input":"2023-04-06T08:52:37.145696Z","iopub.status.idle":"2023-04-06T08:52:44.589289Z","shell.execute_reply.started":"2023-04-06T08:52:37.145654Z","shell.execute_reply":"2023-04-06T08:52:44.588125Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_data, val_data, test_data = get_supervised_data(percentage_labelled, percentage_validation, percentage_test, img_resize=64)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T08:52:44.590776Z","iopub.execute_input":"2023-04-06T08:52:44.592037Z","iopub.status.idle":"2023-04-06T08:54:51.903338Z","shell.execute_reply.started":"2023-04-06T08:52:44.592003Z","shell.execute_reply":"2023-04-06T08:54:51.902184Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Loaded 7393 images\n","output_type":"stream"}]},{"cell_type":"code","source":"main(HYPERPARAMETER_SPACE)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T09:45:02.548523Z","iopub.execute_input":"2023-04-06T09:45:02.551564Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div class=\"tuneStatus\">\n  <div style=\"display: flex;flex-direction: row\">\n    <div style=\"display: flex;flex-direction: column;\">\n      <h3>Tune Status</h3>\n      <table>\n<tbody>\n<tr><td>Current time:</td><td>2023-04-06 13:32:55</td></tr>\n<tr><td>Running for: </td><td>03:47:52.70        </td></tr>\n<tr><td>Memory:      </td><td>10.6/15.6 GiB      </td></tr>\n</tbody>\n</table>\n    </div>\n    <div class=\"vDivider\"></div>\n    <div class=\"systemInfo\">\n      <h3>System Info</h3>\n      Using FIFO scheduling algorithm.<br>Resources requested: 2.0/2 CPUs, 2.0/2 GPUs, 0.0/6.91 GiB heap, 0.0/3.45 GiB objects (0.0/1.0 accelerator_type:T4)\n    </div>\n    <div class=\"vDivider\"></div>\n<div class=\"messages\">\n  <h3>Messages</h3>\n  \n  ... 16 more trials not shown (16 TERMINATED)\n  \n</div>\n<style>\n.messages {\n  color: var(--jp-ui-font-color1);\n  display: flex;\n  flex-direction: column;\n  padding-left: 1em;\n  overflow-y: auto;\n}\n.messages h3 {\n  font-weight: bold;\n}\n.vDivider {\n  border-left-width: var(--jp-border-width);\n  border-left-color: var(--jp-border-color0);\n  border-left-style: solid;\n  margin: 0.5em 1em 0.5em 1em;\n}\n</style>\n\n  </div>\n  <div class=\"hDivider\"></div>\n  <div class=\"trialStatus\">\n    <h3>Trial Status</h3>\n    <table>\n<thead>\n<tr><th>Trial name    </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  depth</th><th style=\"text-align: right;\">  gaussian_noise</th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  lr_decay</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  validation_iou</th><th style=\"text-align: right;\">  running_loss</th><th style=\"text-align: right;\">  train_iou</th></tr>\n</thead>\n<tbody>\n<tr><td>train_746ffe75</td><td>RUNNING   </td><td>172.19.2.2:29118</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">            0.1 </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      1   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           </td></tr>\n<tr><td>train_a1fdf242</td><td>RUNNING   </td><td>172.19.2.2:29198</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">            0.01</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">      0.9 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">              </td><td style=\"text-align: right;\">           </td></tr>\n<tr><td>train_e226b1af</td><td>RUNNING   </td><td>172.19.2.2:28142</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">            1   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      0.9 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         603.387</td><td style=\"text-align: right;\">        0.537167</td><td style=\"text-align: right;\">      14.0193 </td><td style=\"text-align: right;\">   0.535163</td></tr>\n<tr><td>train_fb8a5eb5</td><td>RUNNING   </td><td>172.19.2.2:26177</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">            0.01</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      0.9 </td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">        1775.46 </td><td style=\"text-align: right;\">        0.734488</td><td style=\"text-align: right;\">       6.12714</td><td style=\"text-align: right;\">   0.761856</td></tr>\n<tr><td>train_0020b427</td><td>TERMINATED</td><td>172.19.2.2:6711 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">            0.1 </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">      1   </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         578.304</td><td style=\"text-align: right;\">        0.417831</td><td style=\"text-align: right;\">      24.4367 </td><td style=\"text-align: right;\">   0.411811</td></tr>\n<tr><td>train_01af0d6b</td><td>TERMINATED</td><td>172.19.2.2:19909</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">            0.01</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      0.9 </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        3914.56 </td><td style=\"text-align: right;\">        0.768834</td><td style=\"text-align: right;\">       4.08156</td><td style=\"text-align: right;\">   0.843033</td></tr>\n<tr><td>train_0e5291e2</td><td>TERMINATED</td><td>172.19.2.2:23068</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">            0.01</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      0.9 </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        3921.56 </td><td style=\"text-align: right;\">        0.776969</td><td style=\"text-align: right;\">       6.81555</td><td style=\"text-align: right;\">   0.863588</td></tr>\n<tr><td>train_1c59b1db</td><td>TERMINATED</td><td>172.19.2.2:9409 </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">            0.01</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">      0.99</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         958.916</td><td style=\"text-align: right;\">        0.578517</td><td style=\"text-align: right;\">      26.9717 </td><td style=\"text-align: right;\">   0.577565</td></tr>\n<tr><td>train_1e700e63</td><td>TERMINATED</td><td>172.19.2.2:10136</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">           10   </td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         577.368</td><td style=\"text-align: right;\">        0.41788 </td><td style=\"text-align: right;\">       9.99357</td><td style=\"text-align: right;\">   0.411858</td></tr>\n<tr><td>train_293d20c3</td><td>TERMINATED</td><td>172.19.2.2:14669</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">            0.01</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      0.9 </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        3882.86 </td><td style=\"text-align: right;\">        0.759462</td><td style=\"text-align: right;\">       7.21534</td><td style=\"text-align: right;\">   0.848741</td></tr>\n<tr><td>train_2dbc1b70</td><td>TERMINATED</td><td>172.19.2.2:14147</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">           10   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      0.99</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         595.875</td><td style=\"text-align: right;\">        0.535658</td><td style=\"text-align: right;\">      14.3165 </td><td style=\"text-align: right;\">   0.532663</td></tr>\n<tr><td>train_36ba8f37</td><td>TERMINATED</td><td>172.19.2.2:12101</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">           10   </td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         591.568</td><td style=\"text-align: right;\">        0.417845</td><td style=\"text-align: right;\">       9.76775</td><td style=\"text-align: right;\">   0.411822</td></tr>\n<tr><td>train_390cf85c</td><td>TERMINATED</td><td>172.19.2.2:7597 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">            1   </td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">      1   </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         974.249</td><td style=\"text-align: right;\">        0.531242</td><td style=\"text-align: right;\">      14.675  </td><td style=\"text-align: right;\">   0.528426</td></tr>\n<tr><td>train_3c33d87c</td><td>TERMINATED</td><td>172.19.2.2:14413</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">           10   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">      0.99</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         594.575</td><td style=\"text-align: right;\">        0.417831</td><td style=\"text-align: right;\">      12.483  </td><td style=\"text-align: right;\">   0.411811</td></tr>\n<tr><td>train_41eee754</td><td>TERMINATED</td><td>172.19.2.2:22987</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">            0.1 </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      0.9 </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        3928.49 </td><td style=\"text-align: right;\">        0.772216</td><td style=\"text-align: right;\">      11.7462 </td><td style=\"text-align: right;\">   0.821561</td></tr>\n<tr><td>train_4a516a18</td><td>TERMINATED</td><td>172.19.2.2:8846 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">            0.1 </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      1   </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        1924.61 </td><td style=\"text-align: right;\">        0.668076</td><td style=\"text-align: right;\">       4.698  </td><td style=\"text-align: right;\">   0.676159</td></tr>\n<tr><td>train_5af20d3a</td><td>TERMINATED</td><td>172.19.2.2:21626</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">            0.01</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      0.9 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         793.91 </td><td style=\"text-align: right;\">        0.680325</td><td style=\"text-align: right;\">      16.727  </td><td style=\"text-align: right;\">   0.684265</td></tr>\n<tr><td>train_5c5ba4f8</td><td>TERMINATED</td><td>172.19.2.2:11186</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">            0.1 </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         967.67 </td><td style=\"text-align: right;\">        0.69198 </td><td style=\"text-align: right;\">       8.91807</td><td style=\"text-align: right;\">   0.700757</td></tr>\n<tr><td>train_6bce429e</td><td>TERMINATED</td><td>172.19.2.2:7783 </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">           10   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      0.99</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         580.752</td><td style=\"text-align: right;\">        0.541421</td><td style=\"text-align: right;\">      28.2064 </td><td style=\"text-align: right;\">   0.538213</td></tr>\n<tr><td>train_6f19dbeb</td><td>TERMINATED</td><td>172.19.2.2:27171</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">            0.1 </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">      0.9 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         597.37 </td><td style=\"text-align: right;\">        0.417831</td><td style=\"text-align: right;\">      24.4082 </td><td style=\"text-align: right;\">   0.411811</td></tr>\n</tbody>\n</table>\n  </div>\n</div>\n<style>\n.tuneStatus {\n  color: var(--jp-ui-font-color1);\n}\n.tuneStatus .systemInfo {\n  display: flex;\n  flex-direction: column;\n}\n.tuneStatus td {\n  white-space: nowrap;\n}\n.tuneStatus .trialStatus {\n  display: flex;\n  flex-direction: column;\n}\n.tuneStatus h3 {\n  font-weight: bold;\n}\n.tuneStatus .hDivider {\n  border-bottom-width: var(--jp-border-width);\n  border-bottom-color: var(--jp-border-color0);\n  border-bottom-style: solid;\n}\n.tuneStatus .vDivider {\n  border-left-width: var(--jp-border-width);\n  border-left-color: var(--jp-border-color0);\n  border-left-style: solid;\n  margin: 0.5em 1em 0.5em 1em;\n}\n</style>\n"},"metadata":{}},{"name":"stderr","text":"\u001b[2m\u001b[36m(train pid=6618)\u001b[0m E0406 09:45:42.261101795    6675 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=6487)\u001b[0m E0406 09:46:43.021408058    6528 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div class=\"trialProgress\">\n  <h3>Trial Progress</h3>\n  <table>\n<thead>\n<tr><th>Trial name    </th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>experiment_tag                                                          </th><th>hostname    </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip   </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  running_loss</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  train_accuracy</th><th style=\"text-align: right;\">  train_iou</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th><th style=\"text-align: right;\">  validation_accuracy</th><th style=\"text-align: right;\">  validation_iou</th><th style=\"text-align: right;\">  warmup_time</th></tr>\n</thead>\n<tbody>\n<tr><td>train_0020b427</td><td>2023-04-06_09-55-27</td><td>True  </td><td>                </td><td>8c7e7ced407f433c87d107ba119a6a3f</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         3</td><td>172.19.2.2</td><td style=\"text-align: right;\"> 6711</td><td style=\"text-align: right;\">      24.4367 </td><td style=\"text-align: right;\">             578.304</td><td style=\"text-align: right;\">           197.155</td><td style=\"text-align: right;\">       578.304</td><td style=\"text-align: right;\"> 1680774927</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.411811</td><td style=\"text-align: right;\">   0.411811</td><td style=\"text-align: right;\">                   3</td><td>0020b427  </td><td style=\"text-align: right;\">             0.417831</td><td style=\"text-align: right;\">        0.417831</td><td style=\"text-align: right;\">   0.0252821 </td></tr>\n<tr><td>train_01af0d6b</td><td>2023-04-06_13-00-54</td><td>True  </td><td>                </td><td>12135f0a5d7e4893888e1b9905b85942</td><td>26_batch_size=32,depth=4,gaussian_noise=0.0100,lr=0.0010,lr_decay=0.9000</td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                        20</td><td>172.19.2.2</td><td style=\"text-align: right;\">19909</td><td style=\"text-align: right;\">       4.08156</td><td style=\"text-align: right;\">            3914.56 </td><td style=\"text-align: right;\">           192.965</td><td style=\"text-align: right;\">      3914.56 </td><td style=\"text-align: right;\"> 1680786054</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.92847 </td><td style=\"text-align: right;\">   0.843033</td><td style=\"text-align: right;\">                  20</td><td>01af0d6b  </td><td style=\"text-align: right;\">             0.887408</td><td style=\"text-align: right;\">        0.768834</td><td style=\"text-align: right;\">   0.0315158 </td></tr>\n<tr><td>train_0e5291e2</td><td>2023-04-06_13-31-58</td><td>True  </td><td>                </td><td>0abd72ff1774442985e246729c24ea68</td><td>31_batch_size=16,depth=5,gaussian_noise=0.0100,lr=0.0010,lr_decay=0.9000</td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                        20</td><td>172.19.2.2</td><td style=\"text-align: right;\">23068</td><td style=\"text-align: right;\">       6.81555</td><td style=\"text-align: right;\">            3921.56 </td><td style=\"text-align: right;\">           200.99 </td><td style=\"text-align: right;\">      3921.56 </td><td style=\"text-align: right;\"> 1680787918</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.939098</td><td style=\"text-align: right;\">   0.863588</td><td style=\"text-align: right;\">                  20</td><td>0e5291e2  </td><td style=\"text-align: right;\">             0.893951</td><td style=\"text-align: right;\">        0.776969</td><td style=\"text-align: right;\">   0.0183592 </td></tr>\n<tr><td>train_1c59b1db</td><td>2023-04-06_10-27-22</td><td>True  </td><td>                </td><td>f791ce0806f1470eb38b6ecf520a5e62</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         5</td><td>172.19.2.2</td><td style=\"text-align: right;\"> 9409</td><td style=\"text-align: right;\">      26.9717 </td><td style=\"text-align: right;\">             958.916</td><td style=\"text-align: right;\">           189.324</td><td style=\"text-align: right;\">       958.916</td><td style=\"text-align: right;\"> 1680776842</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.753607</td><td style=\"text-align: right;\">   0.577565</td><td style=\"text-align: right;\">                   5</td><td>1c59b1db  </td><td style=\"text-align: right;\">             0.752983</td><td style=\"text-align: right;\">        0.578517</td><td style=\"text-align: right;\">   0.0129607 </td></tr>\n<tr><td>train_1e700e63</td><td>2023-04-06_10-28-00</td><td>True  </td><td>                </td><td>5f0aa936fe5a43378d97cd02540b3683</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         3</td><td>172.19.2.2</td><td style=\"text-align: right;\">10136</td><td style=\"text-align: right;\">       9.99357</td><td style=\"text-align: right;\">             577.368</td><td style=\"text-align: right;\">           191.753</td><td style=\"text-align: right;\">       577.368</td><td style=\"text-align: right;\"> 1680776880</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.411929</td><td style=\"text-align: right;\">   0.411858</td><td style=\"text-align: right;\">                   3</td><td>1e700e63  </td><td style=\"text-align: right;\">             0.417949</td><td style=\"text-align: right;\">        0.41788 </td><td style=\"text-align: right;\">   0.016789  </td></tr>\n<tr><td>train_293d20c3</td><td>2023-04-06_12-05-52</td><td>True  </td><td>                </td><td>2822163c1f7f459981a37a0b0d1b9be5</td><td>21_batch_size=16,depth=4,gaussian_noise=0.0100,lr=0.0001,lr_decay=0.9000</td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                        20</td><td>172.19.2.2</td><td style=\"text-align: right;\">14669</td><td style=\"text-align: right;\">       7.21534</td><td style=\"text-align: right;\">            3882.86 </td><td style=\"text-align: right;\">           195.266</td><td style=\"text-align: right;\">      3882.86 </td><td style=\"text-align: right;\"> 1680782752</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.931574</td><td style=\"text-align: right;\">   0.848741</td><td style=\"text-align: right;\">                  20</td><td>293d20c3  </td><td style=\"text-align: right;\">             0.88304 </td><td style=\"text-align: right;\">        0.759462</td><td style=\"text-align: right;\">   0.0160482 </td></tr>\n<tr><td>train_2dbc1b70</td><td>2023-04-06_11-07-20</td><td>True  </td><td>                </td><td>83f07480c31e4fdda6325f61874cdb20</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         3</td><td>172.19.2.2</td><td style=\"text-align: right;\">14147</td><td style=\"text-align: right;\">      14.3165 </td><td style=\"text-align: right;\">             595.875</td><td style=\"text-align: right;\">           191.331</td><td style=\"text-align: right;\">       595.875</td><td style=\"text-align: right;\"> 1680779240</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.734074</td><td style=\"text-align: right;\">   0.532663</td><td style=\"text-align: right;\">                   3</td><td>2dbc1b70  </td><td style=\"text-align: right;\">             0.734327</td><td style=\"text-align: right;\">        0.535658</td><td style=\"text-align: right;\">   0.0331132 </td></tr>\n<tr><td>train_36ba8f37</td><td>2023-04-06_10-47-25</td><td>True  </td><td>                </td><td>2b1dd675d5be4bc79229d773d3f214ab</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         3</td><td>172.19.2.2</td><td style=\"text-align: right;\">12101</td><td style=\"text-align: right;\">       9.76775</td><td style=\"text-align: right;\">             591.568</td><td style=\"text-align: right;\">           195.314</td><td style=\"text-align: right;\">       591.568</td><td style=\"text-align: right;\"> 1680778045</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.41184 </td><td style=\"text-align: right;\">   0.411822</td><td style=\"text-align: right;\">                   3</td><td>36ba8f37  </td><td style=\"text-align: right;\">             0.417865</td><td style=\"text-align: right;\">        0.417845</td><td style=\"text-align: right;\">   0.00829268</td></tr>\n<tr><td>train_390cf85c</td><td>2023-04-06_10-11-09</td><td>True  </td><td>                </td><td>a1e7ecfe08934371b02d76d9813200a1</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         5</td><td>172.19.2.2</td><td style=\"text-align: right;\"> 7597</td><td style=\"text-align: right;\">      14.675  </td><td style=\"text-align: right;\">             974.249</td><td style=\"text-align: right;\">           189.664</td><td style=\"text-align: right;\">       974.249</td><td style=\"text-align: right;\"> 1680775869</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.726241</td><td style=\"text-align: right;\">   0.528426</td><td style=\"text-align: right;\">                   5</td><td>390cf85c  </td><td style=\"text-align: right;\">             0.726956</td><td style=\"text-align: right;\">        0.531242</td><td style=\"text-align: right;\">   0.00724649</td></tr>\n<tr><td>train_3c33d87c</td><td>2023-04-06_11-09-38</td><td>True  </td><td>                </td><td>d47c2fc4b27747f482d279be99bf4513</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         3</td><td>172.19.2.2</td><td style=\"text-align: right;\">14413</td><td style=\"text-align: right;\">      12.483  </td><td style=\"text-align: right;\">             594.575</td><td style=\"text-align: right;\">           197.332</td><td style=\"text-align: right;\">       594.575</td><td style=\"text-align: right;\"> 1680779378</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.411811</td><td style=\"text-align: right;\">   0.411811</td><td style=\"text-align: right;\">                   3</td><td>3c33d87c  </td><td style=\"text-align: right;\">             0.417831</td><td style=\"text-align: right;\">        0.417831</td><td style=\"text-align: right;\">   0.0108631 </td></tr>\n<tr><td>train_41eee754</td><td>2023-04-06_13-31-47</td><td>True  </td><td>                </td><td>9dcdd26b05ce46de98b032a1b968050c</td><td>30_batch_size=16,depth=4,gaussian_noise=0.1000,lr=0.0010,lr_decay=0.9000</td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                        20</td><td>172.19.2.2</td><td style=\"text-align: right;\">22987</td><td style=\"text-align: right;\">      11.7462 </td><td style=\"text-align: right;\">            3928.49 </td><td style=\"text-align: right;\">           198.337</td><td style=\"text-align: right;\">      3928.49 </td><td style=\"text-align: right;\"> 1680787907</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.918418</td><td style=\"text-align: right;\">   0.821561</td><td style=\"text-align: right;\">                  20</td><td>41eee754  </td><td style=\"text-align: right;\">             0.890933</td><td style=\"text-align: right;\">        0.772216</td><td style=\"text-align: right;\">   0.0124073 </td></tr>\n<tr><td>train_4a516a18</td><td>2023-04-06_10-37-32</td><td>True  </td><td>                </td><td>ae79e2d37b78422d8a112344ee371b44</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                        10</td><td>172.19.2.2</td><td style=\"text-align: right;\"> 8846</td><td style=\"text-align: right;\">       4.698  </td><td style=\"text-align: right;\">            1924.61 </td><td style=\"text-align: right;\">           195.133</td><td style=\"text-align: right;\">      1924.61 </td><td style=\"text-align: right;\"> 1680777452</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.843646</td><td style=\"text-align: right;\">   0.676159</td><td style=\"text-align: right;\">                  10</td><td>4a516a18  </td><td style=\"text-align: right;\">             0.837213</td><td style=\"text-align: right;\">        0.668076</td><td style=\"text-align: right;\">   0.0317748 </td></tr>\n<tr><td>train_5af20d3a</td><td>2023-04-06_12-26-04</td><td>True  </td><td>                </td><td>d61fec8e6bb544bb8f5c0decd552cbac</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         4</td><td>172.19.2.2</td><td style=\"text-align: right;\">21626</td><td style=\"text-align: right;\">      16.727  </td><td style=\"text-align: right;\">             793.91 </td><td style=\"text-align: right;\">           193.767</td><td style=\"text-align: right;\">       793.91 </td><td style=\"text-align: right;\"> 1680783964</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.829497</td><td style=\"text-align: right;\">   0.684265</td><td style=\"text-align: right;\">                   4</td><td>5af20d3a  </td><td style=\"text-align: right;\">             0.825832</td><td style=\"text-align: right;\">        0.680325</td><td style=\"text-align: right;\">   0.0291915 </td></tr>\n<tr><td>train_5c5ba4f8</td><td>2023-04-06_10-44-22</td><td>True  </td><td>                </td><td>5c40fcea0fc24722bd4820fb97d23833</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         5</td><td>172.19.2.2</td><td style=\"text-align: right;\">11186</td><td style=\"text-align: right;\">       8.91807</td><td style=\"text-align: right;\">             967.67 </td><td style=\"text-align: right;\">           190.889</td><td style=\"text-align: right;\">       967.67 </td><td style=\"text-align: right;\"> 1680777862</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.859724</td><td style=\"text-align: right;\">   0.700757</td><td style=\"text-align: right;\">                   5</td><td>5c5ba4f8  </td><td style=\"text-align: right;\">             0.853134</td><td style=\"text-align: right;\">        0.69198 </td><td style=\"text-align: right;\">   0.0224738 </td></tr>\n<tr><td>train_6bce429e</td><td>2023-04-06_10-05-13</td><td>True  </td><td>                </td><td>76735a6f49214399aedb13dda51c461a</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         3</td><td>172.19.2.2</td><td style=\"text-align: right;\"> 7783</td><td style=\"text-align: right;\">      28.2064 </td><td style=\"text-align: right;\">             580.752</td><td style=\"text-align: right;\">           190.394</td><td style=\"text-align: right;\">       580.752</td><td style=\"text-align: right;\"> 1680775513</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.735273</td><td style=\"text-align: right;\">   0.538213</td><td style=\"text-align: right;\">                   3</td><td>6bce429e  </td><td style=\"text-align: right;\">             0.735443</td><td style=\"text-align: right;\">        0.541421</td><td style=\"text-align: right;\">   0.0101302 </td></tr>\n<tr><td>train_6f19dbeb</td><td>2023-04-06_13-21-29</td><td>True  </td><td>                </td><td>379b095f13b94181857c35d85c91fef8</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         3</td><td>172.19.2.2</td><td style=\"text-align: right;\">27171</td><td style=\"text-align: right;\">      24.4082 </td><td style=\"text-align: right;\">             597.37 </td><td style=\"text-align: right;\">           195.841</td><td style=\"text-align: right;\">       597.37 </td><td style=\"text-align: right;\"> 1680787289</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.411811</td><td style=\"text-align: right;\">   0.411811</td><td style=\"text-align: right;\">                   3</td><td>6f19dbeb  </td><td style=\"text-align: right;\">             0.417831</td><td style=\"text-align: right;\">        0.417831</td><td style=\"text-align: right;\">   0.0240862 </td></tr>\n<tr><td>train_849e0601</td><td>2023-04-06_11-00-38</td><td>True  </td><td>                </td><td>3ed6ff6cb87d49079d00f224c72a89c8</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         4</td><td>172.19.2.2</td><td style=\"text-align: right;\">13170</td><td style=\"text-align: right;\">       7.37835</td><td style=\"text-align: right;\">             778.813</td><td style=\"text-align: right;\">           197.987</td><td style=\"text-align: right;\">       778.813</td><td style=\"text-align: right;\"> 1680778838</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.71496 </td><td style=\"text-align: right;\">   0.546423</td><td style=\"text-align: right;\">                   4</td><td>849e0601  </td><td style=\"text-align: right;\">             0.7178  </td><td style=\"text-align: right;\">        0.551506</td><td style=\"text-align: right;\">   0.0122163 </td></tr>\n<tr><td>train_8801fff1</td><td>2023-04-06_09-55-13</td><td>True  </td><td>                </td><td>26a759f97a984509aeacb12802a9c823</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         3</td><td>172.19.2.2</td><td style=\"text-align: right;\"> 6618</td><td style=\"text-align: right;\">      12.4457 </td><td style=\"text-align: right;\">             580.592</td><td style=\"text-align: right;\">           194.958</td><td style=\"text-align: right;\">       580.592</td><td style=\"text-align: right;\"> 1680774913</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.411811</td><td style=\"text-align: right;\">   0.411811</td><td style=\"text-align: right;\">                   3</td><td>8801fff1  </td><td style=\"text-align: right;\">             0.417831</td><td style=\"text-align: right;\">        0.417831</td><td style=\"text-align: right;\">   0.0158868 </td></tr>\n<tr><td>train_93a9d0d7</td><td>2023-04-06_10-59-28</td><td>True  </td><td>                </td><td>3b507cd017f842a0b1ca1adc322dbe49</td><td>6_batch_size=32,depth=4,gaussian_noise=0.1000,lr=0.0010,lr_decay=0.9000 </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                        20</td><td>172.19.2.2</td><td style=\"text-align: right;\"> 7704</td><td style=\"text-align: right;\">       6.32   </td><td style=\"text-align: right;\">            3851.51 </td><td style=\"text-align: right;\">           194.911</td><td style=\"text-align: right;\">      3851.51 </td><td style=\"text-align: right;\"> 1680778768</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.909285</td><td style=\"text-align: right;\">   0.798137</td><td style=\"text-align: right;\">                  20</td><td>93a9d0d7  </td><td style=\"text-align: right;\">             0.885154</td><td style=\"text-align: right;\">        0.754519</td><td style=\"text-align: right;\">   0.0177379 </td></tr>\n<tr><td>train_9ac69518</td><td>2023-04-06_11-32-28</td><td>True  </td><td>                </td><td>4649b6450d644162994d4afb213bf9b9</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         7</td><td>172.19.2.2</td><td style=\"text-align: right;\">15587</td><td style=\"text-align: right;\">      19.0138 </td><td style=\"text-align: right;\">            1355.06 </td><td style=\"text-align: right;\">           193.478</td><td style=\"text-align: right;\">      1355.06 </td><td style=\"text-align: right;\"> 1680780748</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.846532</td><td style=\"text-align: right;\">   0.68425 </td><td style=\"text-align: right;\">                   7</td><td>9ac69518  </td><td style=\"text-align: right;\">             0.84479 </td><td style=\"text-align: right;\">        0.684784</td><td style=\"text-align: right;\">   0.0106618 </td></tr>\n<tr><td>train_9e11ae33</td><td>2023-04-06_11-00-52</td><td>True  </td><td>                </td><td>ebcc495fef9b43fcad189ae5d637fa4e</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         5</td><td>172.19.2.2</td><td style=\"text-align: right;\">12865</td><td style=\"text-align: right;\">      13.5933 </td><td style=\"text-align: right;\">             974.193</td><td style=\"text-align: right;\">           195.713</td><td style=\"text-align: right;\">       974.193</td><td style=\"text-align: right;\"> 1680778852</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.738224</td><td style=\"text-align: right;\">   0.574672</td><td style=\"text-align: right;\">                   5</td><td>9e11ae33  </td><td style=\"text-align: right;\">             0.740547</td><td style=\"text-align: right;\">        0.579948</td><td style=\"text-align: right;\">   0.00778651</td></tr>\n<tr><td>train_ade2e525</td><td>2023-04-06_09-55-02</td><td>True  </td><td>                </td><td>bb40a017e56f4dac8a83326ca3b7ace2</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         3</td><td>172.19.2.2</td><td style=\"text-align: right;\"> 6529</td><td style=\"text-align: right;\">      14.0476 </td><td style=\"text-align: right;\">             583.942</td><td style=\"text-align: right;\">           192.797</td><td style=\"text-align: right;\">       583.942</td><td style=\"text-align: right;\"> 1680774902</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.727591</td><td style=\"text-align: right;\">   0.517714</td><td style=\"text-align: right;\">                   3</td><td>ade2e525  </td><td style=\"text-align: right;\">             0.728437</td><td style=\"text-align: right;\">        0.521823</td><td style=\"text-align: right;\">   0.0142438 </td></tr>\n<tr><td>train_ae00681a</td><td>2023-04-06_12-26-15</td><td>True  </td><td>                </td><td>e4ab0029442e4db29901ce6681409ce1</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         3</td><td>172.19.2.2</td><td style=\"text-align: right;\">22013</td><td style=\"text-align: right;\">      27.9081 </td><td style=\"text-align: right;\">             593.681</td><td style=\"text-align: right;\">           197.795</td><td style=\"text-align: right;\">       593.681</td><td style=\"text-align: right;\"> 1680783975</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.731   </td><td style=\"text-align: right;\">   0.549991</td><td style=\"text-align: right;\">                   3</td><td>ae00681a  </td><td style=\"text-align: right;\">             0.732714</td><td style=\"text-align: right;\">        0.554342</td><td style=\"text-align: right;\">   0.0216362 </td></tr>\n<tr><td>train_d19261f5</td><td>2023-04-06_11-10-57</td><td>True  </td><td>                </td><td>3116bd59da4a40eeb67391707a763133</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         3</td><td>172.19.2.2</td><td style=\"text-align: right;\">14580</td><td style=\"text-align: right;\">      14.2675 </td><td style=\"text-align: right;\">             604.879</td><td style=\"text-align: right;\">           202.116</td><td style=\"text-align: right;\">       604.879</td><td style=\"text-align: right;\"> 1680779457</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.726522</td><td style=\"text-align: right;\">   0.538028</td><td style=\"text-align: right;\">                   3</td><td>d19261f5  </td><td style=\"text-align: right;\">             0.728625</td><td style=\"text-align: right;\">        0.542957</td><td style=\"text-align: right;\">   0.0118737 </td></tr>\n<tr><td>train_d4fa745d</td><td>2023-04-06_10-18-09</td><td>True  </td><td>                </td><td>5bc5ecc518d04a3e9f2b7f2d31126f76</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         7</td><td>172.19.2.2</td><td style=\"text-align: right;\"> 7876</td><td style=\"text-align: right;\">      16.8596 </td><td style=\"text-align: right;\">            1339.55 </td><td style=\"text-align: right;\">           189.284</td><td style=\"text-align: right;\">      1339.55 </td><td style=\"text-align: right;\"> 1680776289</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.853813</td><td style=\"text-align: right;\">   0.695624</td><td style=\"text-align: right;\">                   7</td><td>d4fa745d  </td><td style=\"text-align: right;\">             0.85089 </td><td style=\"text-align: right;\">        0.694161</td><td style=\"text-align: right;\">   0.0159676 </td></tr>\n<tr><td>train_dad2a0bb</td><td>2023-04-06_09-54-40</td><td>True  </td><td>                </td><td>462ff72d49c44958927a5d363d868f79</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         3</td><td>172.19.2.2</td><td style=\"text-align: right;\"> 6487</td><td style=\"text-align: right;\">      49.9655 </td><td style=\"text-align: right;\">             569.848</td><td style=\"text-align: right;\">           184.812</td><td style=\"text-align: right;\">       569.848</td><td style=\"text-align: right;\"> 1680774880</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.411811</td><td style=\"text-align: right;\">   0.411811</td><td style=\"text-align: right;\">                   3</td><td>dad2a0bb  </td><td style=\"text-align: right;\">             0.417831</td><td style=\"text-align: right;\">        0.417831</td><td style=\"text-align: right;\">   0.00492644</td></tr>\n<tr><td>train_decad4c7</td><td>2023-04-06_12-16-08</td><td>True  </td><td>                </td><td>424a3590a6494a21ab5b7c188b7107a5</td><td>24_batch_size=16,depth=4,gaussian_noise=0.0100,lr=0.0010,lr_decay=0.9000</td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                        20</td><td>172.19.2.2</td><td style=\"text-align: right;\">15785</td><td style=\"text-align: right;\">       7.24812</td><td style=\"text-align: right;\">            3896.77 </td><td style=\"text-align: right;\">           199.32 </td><td style=\"text-align: right;\">      3896.77 </td><td style=\"text-align: right;\"> 1680783368</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.927722</td><td style=\"text-align: right;\">   0.839423</td><td style=\"text-align: right;\">                  20</td><td>decad4c7  </td><td style=\"text-align: right;\">             0.889404</td><td style=\"text-align: right;\">        0.767725</td><td style=\"text-align: right;\">   0.031688  </td></tr>\n<tr><td>train_e082ca57</td><td>2023-04-06_12-12-35</td><td>True  </td><td>                </td><td>e5c500f752ae4d1c929765cbfd5cd54e</td><td>22_batch_size=16,depth=4,gaussian_noise=0.1000,lr=0.0010,lr_decay=0.9000</td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                        20</td><td>172.19.2.2</td><td style=\"text-align: right;\">15317</td><td style=\"text-align: right;\">      11.1263 </td><td style=\"text-align: right;\">            3899.72 </td><td style=\"text-align: right;\">           192.722</td><td style=\"text-align: right;\">      3899.72 </td><td style=\"text-align: right;\"> 1680783155</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.917967</td><td style=\"text-align: right;\">   0.820318</td><td style=\"text-align: right;\">                  20</td><td>e082ca57  </td><td style=\"text-align: right;\">             0.890381</td><td style=\"text-align: right;\">        0.771189</td><td style=\"text-align: right;\">   0.0061059 </td></tr>\n<tr><td>train_e226b1af</td><td>2023-04-06_13-31-49</td><td>False </td><td>                </td><td>bb06885d54904c52afdcd3c83c1b5507</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         3</td><td>172.19.2.2</td><td style=\"text-align: right;\">28142</td><td style=\"text-align: right;\">      14.0193 </td><td style=\"text-align: right;\">             603.387</td><td style=\"text-align: right;\">           200.868</td><td style=\"text-align: right;\">       603.387</td><td style=\"text-align: right;\"> 1680787909</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.747542</td><td style=\"text-align: right;\">   0.535163</td><td style=\"text-align: right;\">                   3</td><td>e226b1af  </td><td style=\"text-align: right;\">             0.74663 </td><td style=\"text-align: right;\">        0.537167</td><td style=\"text-align: right;\">   0.0152218 </td></tr>\n<tr><td>train_e67e3f98</td><td>2023-04-06_11-55-25</td><td>True  </td><td>                </td><td>781c93db1c664affa453e364ca0f973e</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         7</td><td>172.19.2.2</td><td style=\"text-align: right;\">17779</td><td style=\"text-align: right;\">      18.9675 </td><td style=\"text-align: right;\">            1361.73 </td><td style=\"text-align: right;\">           195.064</td><td style=\"text-align: right;\">      1361.73 </td><td style=\"text-align: right;\"> 1680782125</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.846214</td><td style=\"text-align: right;\">   0.686198</td><td style=\"text-align: right;\">                   7</td><td>e67e3f98  </td><td style=\"text-align: right;\">             0.844164</td><td style=\"text-align: right;\">        0.686469</td><td style=\"text-align: right;\">   0.0177605 </td></tr>\n<tr><td>train_f448a576</td><td>2023-04-06_13-11-17</td><td>True  </td><td>                </td><td>f5bb372ad5664f72bdf33bd6341062ae</td><td>27_batch_size=16,depth=4,gaussian_noise=0.1000,lr=0.0001,lr_decay=0.9000</td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                        20</td><td>172.19.2.2</td><td style=\"text-align: right;\">20949</td><td style=\"text-align: right;\">      11.4216 </td><td style=\"text-align: right;\">            3909.44 </td><td style=\"text-align: right;\">           195.276</td><td style=\"text-align: right;\">      3909.44 </td><td style=\"text-align: right;\"> 1680786677</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.906307</td><td style=\"text-align: right;\">   0.799559</td><td style=\"text-align: right;\">                  20</td><td>f448a576  </td><td style=\"text-align: right;\">             0.875361</td><td style=\"text-align: right;\">        0.746314</td><td style=\"text-align: right;\">   0.0110903 </td></tr>\n<tr><td>train_f74e3a53</td><td>2023-04-06_10-37-19</td><td>True  </td><td>                </td><td>75c729e2b89e4b7181dc5e741921b331</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         3</td><td>172.19.2.2</td><td style=\"text-align: right;\">11044</td><td style=\"text-align: right;\">      24.2057 </td><td style=\"text-align: right;\">             582.879</td><td style=\"text-align: right;\">           189.993</td><td style=\"text-align: right;\">       582.879</td><td style=\"text-align: right;\"> 1680777439</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.411811</td><td style=\"text-align: right;\">   0.411811</td><td style=\"text-align: right;\">                   3</td><td>f74e3a53  </td><td style=\"text-align: right;\">             0.417831</td><td style=\"text-align: right;\">        0.417831</td><td style=\"text-align: right;\">   0.0320871 </td></tr>\n<tr><td>train_f9f33846</td><td>2023-04-06_10-57-10</td><td>True  </td><td>                </td><td>e732e3335d8d493a9bc35fdf30c649c8</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         6</td><td>172.19.2.2</td><td style=\"text-align: right;\">12174</td><td style=\"text-align: right;\">       4.34983</td><td style=\"text-align: right;\">            1159.83 </td><td style=\"text-align: right;\">           189.885</td><td style=\"text-align: right;\">      1159.83 </td><td style=\"text-align: right;\"> 1680778630</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.852865</td><td style=\"text-align: right;\">   0.684271</td><td style=\"text-align: right;\">                   6</td><td>f9f33846  </td><td style=\"text-align: right;\">             0.849807</td><td style=\"text-align: right;\">        0.683   </td><td style=\"text-align: right;\">   0.0107534 </td></tr>\n<tr><td>train_fb8a5eb5</td><td>2023-04-06_13-30-45</td><td>False </td><td>                </td><td>df5432aac96c44baa97a1ce428ceb817</td><td>                                                                        </td><td>fe5c90fb383f</td><td style=\"text-align: right;\">                         9</td><td>172.19.2.2</td><td style=\"text-align: right;\">26177</td><td style=\"text-align: right;\">       6.12714</td><td style=\"text-align: right;\">            1775.46 </td><td style=\"text-align: right;\">           199.245</td><td style=\"text-align: right;\">      1775.46 </td><td style=\"text-align: right;\"> 1680787845</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">        0.885696</td><td style=\"text-align: right;\">   0.761856</td><td style=\"text-align: right;\">                   9</td><td>fb8a5eb5  </td><td style=\"text-align: right;\">             0.86912 </td><td style=\"text-align: right;\">        0.734488</td><td style=\"text-align: right;\">   0.0208035 </td></tr>\n</tbody>\n</table>\n</div>\n<style>\n.trialProgress {\n  display: flex;\n  flex-direction: column;\n  color: var(--jp-ui-font-color1);\n}\n.trialProgress h3 {\n  font-weight: bold;\n}\n.trialProgress td {\n  white-space: nowrap;\n}\n</style>\n"},"metadata":{}},{"name":"stderr","text":"\u001b[2m\u001b[36m(train pid=6529)\u001b[0m E0406 09:51:50.165680686    6587 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=6487)\u001b[0m E0406 09:52:55.973460430    6528 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7597)\u001b[0m E0406 09:55:01.310143592    7654 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7876)\u001b[0m E0406 09:55:54.116057569    7935 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7597)\u001b[0m E0406 09:57:55.182848627    7654 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7704)\u001b[0m E0406 10:00:03.867253715    7747 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7783)\u001b[0m E0406 10:00:18.499803482    7843 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7876)\u001b[0m E0406 10:05:28.013877670    7935 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7597)\u001b[0m E0406 10:06:10.936201942    7654 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7876)\u001b[0m E0406 10:06:52.880381718    7935 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7876)\u001b[0m E0406 10:08:07.255853848    7935 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7597)\u001b[0m E0406 10:10:39.213563117    7654 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7704)\u001b[0m E0406 10:11:25.884823695    7747 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=9409)\u001b[0m E0406 10:11:28.542728214    9465 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=8846)\u001b[0m E0406 10:14:33.683230469    8889 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7876)\u001b[0m E0406 10:14:59.810074208    7935 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7876)\u001b[0m E0406 10:16:24.086343547    7935 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7704)\u001b[0m E0406 10:20:26.034455372    7747 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=10136)\u001b[0m E0406 10:21:07.353675751   10178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=8846)\u001b[0m E0406 10:21:33.702248240    8889 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7704)\u001b[0m E0406 10:24:05.568467554    7747 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=11044)\u001b[0m E0406 10:29:11.883128392   11098 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=11044)\u001b[0m E0406 10:32:21.460558940   11098 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=8846)\u001b[0m E0406 10:33:46.386188331    8889 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7704)\u001b[0m E0406 10:36:21.034574797    7747 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=12101)\u001b[0m E0406 10:37:43.849306962   12142 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=12174)\u001b[0m E0406 10:37:56.811074772   12224 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=12174)\u001b[0m E0406 10:39:19.325915123   12224 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=12174)\u001b[0m E0406 10:40:36.071679955   12224 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=12101)\u001b[0m E0406 10:46:54.395248058   12142 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=13170)\u001b[0m E0406 10:47:45.344537189   13215 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=12865)\u001b[0m E0406 10:49:21.936289610   12907 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=7704)\u001b[0m E0406 10:53:02.791605448    7747 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=12174)\u001b[0m E0406 10:56:40.252702253   12224 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=12865)\u001b[0m E0406 10:58:58.350163957   12907 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=14147)\u001b[0m E0406 11:00:14.988866139   14188 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=14147)\u001b[0m E0406 11:00:52.231169530   14188 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=14580)\u001b[0m E0406 11:01:00.450333569   14621 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=14669)\u001b[0m E0406 11:02:42.144021657   14719 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=14147)\u001b[0m E0406 11:03:39.251691440   14188 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15317)\u001b[0m E0406 11:07:41.800280194   15357 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15317)\u001b[0m E0406 11:11:11.333367834   15357 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15785)\u001b[0m E0406 11:12:44.677932849   15827 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=14669)\u001b[0m E0406 11:13:41.616282441   14719 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15785)\u001b[0m E0406 11:14:33.377204174   15827 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15785)\u001b[0m E0406 11:15:58.098463030   15827 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15785)\u001b[0m E0406 11:21:01.144628516   15827 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15587)\u001b[0m E0406 11:22:20.094242449   15631 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15317)\u001b[0m E0406 11:24:05.536708710   15357 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15317)\u001b[0m E0406 11:25:30.773686778   15357 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=14669)\u001b[0m E0406 11:26:33.646234844   14719 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15785)\u001b[0m E0406 11:26:54.507129896   15827 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15317)\u001b[0m E0406 11:28:41.641154336   15357 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=14669)\u001b[0m E0406 11:31:40.866746823   14719 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15317)\u001b[0m E0406 11:33:14.588379946   15357 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15785)\u001b[0m E0406 11:35:21.676568660   15827 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=14669)\u001b[0m E0406 11:39:32.672216594   14719 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=14669)\u001b[0m E0406 11:40:03.854841406   14719 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15785)\u001b[0m E0406 11:41:49.603024693   15827 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=17779)\u001b[0m E0406 11:42:35.092026127   17820 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=17779)\u001b[0m E0406 11:43:56.407335893   17820 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=14669)\u001b[0m E0406 11:44:42.146684675   14719 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15317)\u001b[0m E0406 11:44:53.823306681   15357 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15785)\u001b[0m E0406 11:45:02.293235298   15827 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15785)\u001b[0m E0406 11:54:41.479592867   15827 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15785)\u001b[0m E0406 11:56:35.387921272   15827 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15785)\u001b[0m E0406 11:59:16.254944044   15827 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15317)\u001b[0m E0406 11:59:40.194302678   15357 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15317)\u001b[0m E0406 12:02:21.567058829   15357 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=19909)\u001b[0m E0406 12:04:55.191880950   19951 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=15317)\u001b[0m E0406 12:09:22.953522890   15357 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=20949)\u001b[0m E0406 12:12:14.569448828   20991 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=20949)\u001b[0m E0406 12:12:50.369985514   20991 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=20949)\u001b[0m E0406 12:15:34.559194389   20991 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=21626)\u001b[0m E0406 12:16:21.864601495   21668 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22013)\u001b[0m E0406 12:19:12.479064377   22056 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=20949)\u001b[0m E0406 12:22:02.206870775   20991 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=21626)\u001b[0m E0406 12:22:19.294057542   21668 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=20949)\u001b[0m E0406 12:27:17.574166852   20991 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22987)\u001b[0m E0406 12:29:47.044053532   23027 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22987)\u001b[0m E0406 12:31:10.847043133   23027 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=19909)\u001b[0m E0406 12:31:39.504906424   19951 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22987)\u001b[0m E0406 12:33:00.882042835   23027 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=20949)\u001b[0m E0406 12:33:47.633779833   20991 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22987)\u001b[0m E0406 12:34:24.839139693   23027 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=19909)\u001b[0m E0406 12:37:41.919392928   19951 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=19909)\u001b[0m E0406 12:39:35.501906525   19951 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=20949)\u001b[0m E0406 12:40:21.945618686   20991 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=19909)\u001b[0m E0406 12:41:00.298717573   19951 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=23068)\u001b[0m E0406 12:45:49.293332445   23135 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=23068)\u001b[0m E0406 12:46:21.862450598   23135 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=19909)\u001b[0m E0406 12:47:29.109245660   19951 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22987)\u001b[0m E0406 12:50:40.185036254   23027 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=20949)\u001b[0m E0406 12:51:23.724658702   20991 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22987)\u001b[0m E0406 12:51:57.136732793   23027 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=19909)\u001b[0m E0406 12:52:33.142628770   19951 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=20949)\u001b[0m E0406 12:54:35.392045483   20991 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22987)\u001b[0m E0406 12:55:11.575067055   23027 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=23068)\u001b[0m E0406 12:55:26.817693700   23135 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=19909)\u001b[0m E0406 12:55:46.402654578   19951 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=19909)\u001b[0m E0406 12:57:10.828275181   19951 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=23068)\u001b[0m E0406 12:57:20.092175318   23135 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=23068)\u001b[0m E0406 13:00:31.968686772   23135 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=26177)\u001b[0m E0406 13:01:16.181725477   26233 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22987)\u001b[0m E0406 13:04:55.891424580   23027 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22987)\u001b[0m E0406 13:06:52.543176177   23027 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=23068)\u001b[0m E0406 13:08:53.522822639   23135 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=23068)\u001b[0m E0406 13:14:55.048471821   23135 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22987)\u001b[0m E0406 13:16:43.832640443   23027 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22987)\u001b[0m E0406 13:18:01.236085147   23027 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=27171)\u001b[0m E0406 13:18:14.191045769   27223 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22987)\u001b[0m E0406 13:19:57.959962736   23027 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=23068)\u001b[0m E0406 13:20:10.696302371   23135 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=27171)\u001b[0m E0406 13:20:58.745115897   27223 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=23068)\u001b[0m E0406 13:21:28.067180125   23135 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=26177)\u001b[0m E0406 13:22:13.760212914   26233 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22987)\u001b[0m E0406 13:23:20.396891147   23027 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=23068)\u001b[0m E0406 13:24:47.686072284   23135 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=28142)\u001b[0m E0406 13:25:11.870290083   28192 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=23068)\u001b[0m E0406 13:25:20.590942983   23135 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=22987)\u001b[0m E0406 13:26:38.092998460   23027 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=26177)\u001b[0m E0406 13:28:49.039965772   26233 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n\u001b[2m\u001b[36m(train pid=26177)\u001b[0m E0406 13:30:15.172236209   26233 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2023-04-06T09:09:06.223014Z","iopub.execute_input":"2023-04-06T09:09:06.224624Z","iopub.status.idle":"2023-04-06T09:09:07.665071Z","shell.execute_reply.started":"2023-04-06T09:09:06.224548Z","shell.execute_reply":"2023-04-06T09:09:07.663566Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"E0406 09:09:06.233536414      23 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n","output_type":"stream"},{"name":"stdout","text":"Models\t__notebook_source__.ipynb\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}