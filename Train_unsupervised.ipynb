{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import model_UNet\n",
    "from data_augmentation import augmentation, colorjiter, invert\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocessing_1_dataloader import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weigth coef for the Unsupervised\n",
    "def dice_loss(logits, targets): \n",
    "  preds_animal = F.softmax(logits, dim=1)\n",
    "  targets_animal = torch.squeeze(targets)\n",
    "  preds_animal = preds_animal[:,1,:,:]\n",
    "  eps = 1e-6\n",
    "  intersection = (preds_animal * targets_animal).sum()\n",
    "  dice_coef = (2. * intersection + eps) / ((preds_animal**2).sum() + eps)\n",
    "  dice_loss = 1 - dice_coef\n",
    "  return dice_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def wt(rampup_length, current, alpha, wait_period = 5):\n",
    "\n",
    "  if current < wait_period:\n",
    "    return 0.0\n",
    "    \n",
    "  else:\n",
    "    if rampup_length == 0:\n",
    "                return 1.0\n",
    "    else:\n",
    "        current = np.clip(current, 0.0, rampup_length)\n",
    "        phase = 1.0 - current / rampup_length\n",
    "        return float(alpha * np.exp(-5.0 * phase * phase))\n",
    "\n",
    "\n",
    "#update the Teacher weigth\n",
    "@torch.no_grad()\n",
    "def update_ema_variables(model, ema_model, alpha, global_step): \n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
    "\n",
    "@torch.no_grad()\n",
    "def unsup_loss(z, modelT, imgs, unsup_crit): \n",
    "\n",
    "  imgT_aug = augmentation(imgs).type(torch.float32)\n",
    "  imgT_aug = imgT_aug.to(device)\n",
    "\n",
    "  # z_bar will act as pseudo-labels\n",
    "  z_bar = modelT(imgT_aug)\n",
    "  z_bar = F.softmax(z_bar, dim = 1)\n",
    "  z_bar_preds = torch.argmax(z_bar, dim=1)\n",
    "\n",
    "  # Transform z_bar into predictions\n",
    "  Lu = unsup_crit(z, z_bar_preds).to(device)\n",
    "\n",
    "  return Lu\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, device):\n",
    "  \n",
    "  model.eval()\n",
    "  intersection_total, union_total = 0, 0\n",
    "  pixel_correct, pixel_count = 0, 0\n",
    "    \n",
    "  for data in dataloader:\n",
    "    imgs, labels = data\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "    logits = model(imgs)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    targets = torch.squeeze(labels)\n",
    "            \n",
    "    intersection_total += torch.logical_and(preds, targets).sum()\n",
    "    union_total += torch.logical_or(preds, targets).sum()\n",
    "            \n",
    "    pixel_correct += (preds == targets).sum()\n",
    "    pixel_count += targets.numel()\n",
    "\n",
    "  iou = (intersection_total / union_total).item()\n",
    "  accuracy = (pixel_correct / pixel_count).item()\n",
    "  \n",
    "  model.train()\n",
    "  return accuracy, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "#### Hyper-Param ####\n",
    "\n",
    "# device to use\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") #Enable GPU support\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# data parms\n",
    "supervised_percent = 0.2  # what percent of training is to be labelled\n",
    "img_resize = 64             # resize all images to this size \n",
    "is_only_labelled = False     # Training only on supervised data or not\n",
    "\n",
    "# model params\n",
    "depth = 3       # depth of unet\n",
    "\n",
    "# Training params\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "ramp_up = 5\n",
    "consistency = 56\n",
    "alpha = 0.999\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martaemili/Desktop/COMP0197 Group CW/model_UNet.py:211: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  init.xavier_normal(m.weight)\n",
      "/Users/martaemili/Desktop/COMP0197 Group CW/model_UNet.py:212: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(m.bias, 0)\n"
     ]
    }
   ],
   "source": [
    "#### Initialisation ####\n",
    "#create 2 network\n",
    "modelS = model_UNet.UNet(in_channels=3, num_classes=2, depth=depth)\n",
    "modelS = modelS.to(device)\n",
    "modelT = model_UNet.UNet(in_channels=3, num_classes=2, depth=depth)\n",
    "modelT = modelT.to(device)\n",
    "#create the losses\n",
    "sup_crit = nn.CrossEntropyLoss().to(device)\n",
    "unsup_crit = nn.CrossEntropyLoss().to(device)\n",
    "#optimizer\n",
    "optimizer = Adam(modelS.parameters())\n",
    "\n",
    "##data loader\n",
    "mixed_train_loader, val_loader, test_loader = get_data(supervised_percent,1-supervised_percent,0.2,0.1, batch_size=batch_size, img_resize=img_resize, is_mixed_loader=is_only_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 - Loss:  69.97\n",
      "accuracy: 82%; IOU: 63%\n",
      "Epoch:    2 - Loss:  67.06\n",
      "accuracy: 82%; IOU: 67%\n",
      "Epoch:    3 - Loss:  65.43\n",
      "accuracy: 84%; IOU: 69%\n",
      "Epoch:    4 - Loss:  62.54\n",
      "accuracy: 82%; IOU: 61%\n",
      "Epoch:    5 - Loss:  61.81\n",
      "accuracy: 85%; IOU: 71%\n",
      "Epoch:    6 - Loss: 2520.15\n",
      "accuracy: 85%; IOU: 69%\n",
      "Epoch:    7 - Loss: 2533.57\n",
      "accuracy: 86%; IOU: 72%\n",
      "Epoch:    8 - Loss: 2432.82\n",
      "accuracy: 87%; IOU: 73%\n",
      "Epoch:    9 - Loss: 2453.86\n",
      "accuracy: 87%; IOU: 72%\n",
      "Epoch:   10 - Loss: 2382.75\n",
      "accuracy: 87%; IOU: 73%\n",
      "Epoch:   11 - Loss: 2394.56\n",
      "accuracy: 87%; IOU: 72%\n",
      "Epoch:   12 - Loss: 2365.74\n",
      "accuracy: 87%; IOU: 73%\n",
      "Epoch:   13 - Loss: 2363.22\n",
      "accuracy: 87%; IOU: 71%\n",
      "Epoch:   14 - Loss: 2290.71\n",
      "accuracy: 87%; IOU: 75%\n",
      "Epoch:   15 - Loss: 2265.78\n",
      "accuracy: 87%; IOU: 73%\n",
      "Epoch:   16 - Loss: 2241.05\n",
      "accuracy: 87%; IOU: 71%\n",
      "Epoch:   17 - Loss: 2218.72\n",
      "accuracy: 87%; IOU: 74%\n",
      "Epoch:   18 - Loss: 2261.46\n",
      "accuracy: 88%; IOU: 75%\n",
      "Epoch:   19 - Loss: 2197.85\n",
      "accuracy: 88%; IOU: 75%\n",
      "Epoch:   20 - Loss: 2187.41\n",
      "accuracy: 89%; IOU: 76%\n",
      "Epoch:   21 - Loss: 2145.70\n",
      "accuracy: 88%; IOU: 73%\n",
      "Epoch:   22 - Loss: 2094.06\n",
      "accuracy: 86%; IOU: 68%\n",
      "Epoch:   23 - Loss: 2171.45\n",
      "accuracy: 88%; IOU: 74%\n",
      "Epoch:   24 - Loss: 2065.75\n",
      "accuracy: 87%; IOU: 73%\n",
      "Epoch:   25 - Loss: 2141.98\n",
      "accuracy: 87%; IOU: 72%\n",
      "Epoch:   26 - Loss: 2114.09\n",
      "accuracy: 88%; IOU: 76%\n",
      "Epoch:   27 - Loss: 2122.59\n",
      "accuracy: 89%; IOU: 76%\n",
      "Epoch:   28 - Loss: 2054.70\n",
      "accuracy: 88%; IOU: 74%\n",
      "Epoch:   29 - Loss: 2085.01\n",
      "accuracy: 88%; IOU: 74%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m loss \u001b[39m=\u001b[39m Ls \u001b[39m+\u001b[39m w_t \u001b[39m*\u001b[39m Lu\n\u001b[1;32m     40\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 42\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()    \n\u001b[1;32m     43\u001b[0m global_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     44\u001b[0m update_ema_variables(modelS, modelT, alpha, global_step)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/comp0197-cw1-pt/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/comp0197-cw1-pt/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/comp0197-cw1-pt/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/comp0197-cw1-pt/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/comp0197-cw1-pt/lib/python3.10/site-packages/torch/optim/adam.py:412\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m--> 412\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "eval_freq = 1\n",
    "losses, accs, IOUs = [], [], []\n",
    "optimizer = Adam(modelS.parameters())\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "        modelS.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        w_t = wt(ramp_up, epoch, consistency)\n",
    "\n",
    "        for step, data in enumerate(mixed_train_loader):\n",
    "\n",
    "            imgs, labs = data\n",
    "            #Â Augment images\n",
    "            imgS_aug = augmentation(imgs)\n",
    "\n",
    "            imgS_aug = imgS_aug.to(device)\n",
    "            labs = labs.squeeze().type(torch.LongTensor).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass for student and teacher\n",
    "            z = modelS(imgS_aug) \n",
    "\n",
    "            # Find img with label\n",
    "            sup_idx = torch.tensor([(elem != -1).item() for elem in labs[:, 0, 0]]).to(device) #If batchsize is the first dim\n",
    "\n",
    "            assert len(sup_idx) != 0\n",
    "\n",
    "            # Calculate losses\n",
    "            Ls = sup_crit(z[sup_idx], labs[sup_idx])\n",
    "            Lu = unsup_loss(z, modelT, imgs, unsup_crit)\n",
    "  \n",
    "            loss = Ls + w_t * Lu\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()    \n",
    "            global_step += 1\n",
    "            update_ema_variables(modelS, modelT, alpha, global_step)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            #optimizer.param_groups[0]['lr'] = lr(epoch+2)\n",
    "            #print(loss.item())\n",
    "\n",
    "        print(f'Epoch: {epoch + 1:4d} - Loss: {running_loss:6.2f}')\n",
    "        losses.append(running_loss)\n",
    "\n",
    "        if (epoch % eval_freq == 0):\n",
    "          accuracy, IOU = evaluate_model(modelS, val_loader, device)\n",
    "          accs.append(accuracy)\n",
    "          IOUs.append(IOU)\n",
    "          print(f'accuracy: {accuracy:2.0%}; IOU: {IOU:2.0%}')\n",
    "\n",
    "np.savetxt(\"losses\", losses)\n",
    "np.savetxt(\"accs\", accs)\n",
    "np.savetxt(\"IOUs\", IOUs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197-cw1-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
