{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import model_UNet\n",
    "from data_augmentation import augmentation, colorjiter, invert\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocessing_1_dataloader import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#### Hyper-Param ####\n",
    "\n",
    "# device to use\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #Enable GPU support\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# data parms\n",
    "supervised_percent = 0.25  # what percent of training is to be labelled\n",
    "img_resize = 64             # resize all images to this size \n",
    "is_mixed_labels = True     # Training only on supervised data or not\n",
    "\n",
    "# model params\n",
    "depth = 3       # depth of unet\n",
    "dropout_rate = 0.25\n",
    "\n",
    "# Training params\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "ramp_up = 10\n",
    "consistency = 56\n",
    "alpha = 0.999\n",
    "global_step = 0\n",
    "lr = 1e-3\n",
    "lr_gamma = 0.9\n",
    "wait_period = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(12).reshape(3,4)\n",
    "b = torch.argmax(a, dim=0)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weigth coef for the Unsupervised\n",
    "def dice_loss(logits, targets, is_modelT_preds=True): \n",
    "  if is_modelT_preds:\n",
    "    targets_animal = torch.unsqueeze(targets, dim=1)\n",
    "  targets_animal = torch.squeeze(targets)\n",
    "  preds_animal = F.softmax(logits, dim=1)\n",
    "  preds_animal = preds_animal[:,1,:,:]\n",
    "  eps = 1e-6\n",
    "  intersection = (preds_animal * targets_animal).sum()\n",
    "  dice_coef = (2. * intersection + eps) / ((preds_animal**2).sum() + (targets_animal**2).sum() + eps)\n",
    "  dice_loss = 1 - dice_coef\n",
    "  return dice_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def wt(rampup_length, current, alpha, wait_period = 5):\n",
    "\n",
    "  if current < wait_period:\n",
    "    return 0.0\n",
    "    \n",
    "  else:\n",
    "    if rampup_length == 0:\n",
    "                return 1.0\n",
    "    else:\n",
    "        current = np.clip(current, 0.0, rampup_length)\n",
    "        phase = 1.0 - current / rampup_length\n",
    "        return float(alpha * np.exp(-5.0 * phase * phase))\n",
    "\n",
    "\n",
    "#update the Teacher weight\n",
    "@torch.no_grad()\n",
    "def update_ema_variables(model, ema_model, alpha, global_step): \n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
    "\n",
    "\n",
    "def unsup_loss(z, modelT, imgs, unsup_crit): \n",
    "\n",
    "  imgT_aug = augmentation(imgs).type(torch.float32)\n",
    "  imgT_aug = imgT_aug.to(device)\n",
    "\n",
    "  # z_bar will act as pseudo-labels\n",
    "  with torch.no_grad():\n",
    "    z_bar = modelT(imgT_aug)\n",
    "    # z_bar = F.softmax(z_bar, dim = 1)\n",
    "    z_bar_preds = torch.argmax(z_bar, dim=1)\n",
    "\n",
    "  # Transform z_bar into predictions\n",
    "  # Lu = unsup_crit(z, z_bar_preds).to(device)\n",
    "  Lu = dice_loss(z, z_bar_preds, is_modelT_preds=True)\n",
    "\n",
    "  return Lu\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, device):\n",
    "  \n",
    "  model.eval()\n",
    "  intersection_total, union_total = 0, 0\n",
    "  pixel_correct, pixel_count = 0, 0\n",
    "    \n",
    "  for data in dataloader:\n",
    "    imgs, labels = data\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "    logits = model(imgs)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    targets = torch.squeeze(labels)\n",
    "            \n",
    "    intersection_total += torch.logical_and(preds, targets).sum()\n",
    "    union_total += torch.logical_or(preds, targets).sum()\n",
    "            \n",
    "    pixel_correct += (preds == targets).sum()\n",
    "    pixel_count += targets.numel()\n",
    "\n",
    "  iou = (intersection_total / union_total).item()\n",
    "  accuracy = (pixel_correct / pixel_count).item()\n",
    "  \n",
    "  model.train()\n",
    "  return accuracy, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fred/repositories/comp0197-cw2/model_UNet.py:213: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  init.xavier_normal(m.weight)\n",
      "/home/fred/repositories/comp0197-cw2/model_UNet.py:214: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(m.bias, 0)\n"
     ]
    }
   ],
   "source": [
    "#### Initialisation ####\n",
    "#create 2 network\n",
    "modelS = model_UNet.UNet(in_channels=3, num_classes=2, depth=depth)\n",
    "modelS = modelS.to(device)\n",
    "modelT = model_UNet.UNet(in_channels=3, num_classes=2, depth=depth)\n",
    "modelT = modelT.to(device)\n",
    "#create the losses\n",
    "sup_crit = nn.CrossEntropyLoss().to(device)\n",
    "unsup_crit = nn.CrossEntropyLoss().to(device)\n",
    "#optimizer\n",
    "optimizer = Adam(modelS.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=lr_gamma, last_epoch=-1, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/images.tar.gz\n",
      "Using downloaded and verified file: ./data/annotations.tar.gz\n",
      "all images are =  7393\n"
     ]
    }
   ],
   "source": [
    "##data loader\n",
    "mixed_train_loader, val_loader, test_loader = get_data(supervised_percent,1-supervised_percent,0.2,0.1, batch_size=batch_size, img_resize=img_resize, is_mixed_loader=is_mixed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue from a previous trained model\n",
    "\n",
    "# modelS = model_UNet.UNet(num_classes=2, in_channels=3, depth=3)\n",
    "# modelS.load_state_dict(torch.load('models/unet_supervised_depth3_v1'))\n",
    "# modelS.to(device)\n",
    "# optimizer = Adam(modelS.parameters(), lr=lr)\n",
    "# scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=lr_gamma, last_epoch=-1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sup_idx == 0\n",
      "Epoch:    1 - Loss:  44.04, loss_sup:   44.0, loss_unsup:   23.4, w_t:  0.00\n",
      "accuracy: 78%; IOU: 60%\n",
      "Epoch:    2 - Loss:  35.14, loss_sup:   35.1, loss_unsup:   20.6, w_t:  0.00\n",
      "accuracy: 77%; IOU: 61%\n",
      "Epoch:    3 - Loss:  33.87, loss_sup:   33.9, loss_unsup:   17.8, w_t:  0.00\n",
      "accuracy: 80%; IOU: 62%\n",
      "Epoch:    4 - Loss:  33.48, loss_sup:   33.5, loss_unsup:   17.5, w_t:  0.00\n",
      "accuracy: 81%; IOU: 64%\n",
      "Epoch:    5 - Loss:  30.72, loss_sup:   30.7, loss_unsup:   16.8, w_t:  0.00\n",
      "accuracy: 82%; IOU: 67%\n",
      "Epoch:    6 - Loss:  35.62, loss_sup:   31.1, loss_unsup:   15.7, w_t:  0.29\n",
      "accuracy: 82%; IOU: 67%\n",
      "Epoch:    7 - Loss:  36.52, loss_sup:   30.4, loss_unsup:   13.7, w_t:  0.45\n",
      "accuracy: 81%; IOU: 66%\n",
      "Epoch:    8 - Loss:  38.15, loss_sup:   29.8, loss_unsup:   13.2, w_t:  0.64\n",
      "accuracy: 82%; IOU: 67%\n",
      "Epoch:    9 - Loss:  38.95, loss_sup:   29.2, loss_unsup:   11.9, w_t:  0.82\n",
      "accuracy: 82%; IOU: 67%\n",
      "Epoch:   10 - Loss:  38.97, loss_sup:   28.4, loss_unsup:   11.1, w_t:  0.95\n",
      "accuracy: 82%; IOU: 68%\n",
      "Epoch:   11 - Loss:  39.62, loss_sup:   28.6, loss_unsup:   11.0, w_t:  1.00\n",
      "accuracy: 83%; IOU: 68%\n",
      "Epoch:   12 - Loss:  38.75, loss_sup:   27.5, loss_unsup:   11.3, w_t:  1.00\n",
      "accuracy: 83%; IOU: 69%\n",
      "Epoch:   13 - Loss:  41.43, loss_sup:   29.5, loss_unsup:   11.9, w_t:  1.00\n",
      "accuracy: 80%; IOU: 66%\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "eval_freq = 1\n",
    "losses, accs, IOUs = [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "        modelS.train()\n",
    "        running_loss = 0\n",
    "        running_loss_sup = 0\n",
    "        running_loss_unsup = 0\n",
    "\n",
    "        w_t = wt(rampup_length=ramp_up, current=epoch, alpha=alpha, wait_period=wait_period)\n",
    "\n",
    "        for step, data in enumerate(mixed_train_loader):\n",
    "\n",
    "            imgs, labs = data\n",
    "            # Augment images\n",
    "            imgS_aug = augmentation(imgs)\n",
    "\n",
    "            imgS_aug = imgS_aug.to(device)\n",
    "            labs = labs.squeeze().type(torch.LongTensor).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass for student and teacher\n",
    "            z = modelS(imgS_aug) \n",
    "\n",
    "            # Find img with label\n",
    "            sup_idx = np.asarray(torch.tensor([(elem != -1).item() for elem in labs[:, 0, 0]])) #If batchsize is the first dim\n",
    "\n",
    "            if sup_idx.sum() == 0:\n",
    "              print('sup_idx == 0')\n",
    "              Ls = torch.tensor(0)\n",
    "            else:\n",
    "              # Calculate losses\n",
    "              # Ls = sup_crit(z[sup_idx], labs[sup_idx])\n",
    "              Ls = dice_loss(z[sup_idx], labs[sup_idx])\n",
    "            \n",
    "            Lu = unsup_loss(z, modelT, imgs, unsup_crit)\n",
    "    \n",
    "            loss = Ls + w_t * Lu\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()    \n",
    "            global_step += 1\n",
    "            update_ema_variables(modelS, modelT, alpha, global_step)\n",
    "            running_loss += loss.item()\n",
    "            running_loss_sup += Ls.item()\n",
    "            running_loss_unsup += Lu.item()\n",
    "            \n",
    "\n",
    "            #optimizer.param_groups[0]['lr'] = lr(epoch+2)\n",
    "            #print(loss.item())\n",
    "\n",
    "        print(f'Epoch: {epoch + 1:4d} - Loss: {running_loss:6.2f}, loss_sup: {running_loss_sup:6.1f}, loss_unsup: {running_loss_unsup:6.1f}, w_t: {w_t: 3.2f}')\n",
    "        losses.append(running_loss)\n",
    "\n",
    "        if (epoch % eval_freq == 0):\n",
    "          accuracy, IOU = evaluate_model(modelS, val_loader, device)\n",
    "          accs.append(accuracy)\n",
    "          IOUs.append(IOU)\n",
    "          print(f'accuracy: {accuracy:2.0%}; IOU: {IOU:2.0%}')\n",
    "\n",
    "np.savetxt(\"losses\", losses)\n",
    "np.savetxt(\"accs\", accs)\n",
    "np.savetxt(\"IOUs\", IOUs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197-cw1-pt-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10b77cd3e758c5cae7cf43d6678d201973fa77fb3a7bd02539f4da1babd23f0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
