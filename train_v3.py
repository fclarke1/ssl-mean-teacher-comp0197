# -*- coding: utf-8 -*-
"""Train_v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jnRmJwUMyTRJzXbgtzAPvgVf69xtFgVH
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class DoubleConv(nn.Module):
    """(convolution => [BN] => ReLU) * 2"""

    def __init__(self, in_channels, out_channels, mid_channels=None):
        super().__init__()
        if not mid_channels:
            mid_channels = out_channels
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)


class Down(nn.Module):
    """Downscaling with maxpool then double conv"""

    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.maxpool_conv = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(in_channels, out_channels)
        )

    def forward(self, x):
        return self.maxpool_conv(x)


class Up(nn.Module):
    """Upscaling then double conv"""

    def __init__(self, in_channels, out_channels, bilinear=True):
        super().__init__()

        # if bilinear, use the normal convolutions to reduce the number of channels
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)
        else:
            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)
            self.conv = DoubleConv(in_channels, out_channels)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        # input is CHW
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]

        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
                        diffY // 2, diffY - diffY // 2])
        # if you have padding issues, see
        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a
        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)


class OutConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(OutConv, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        return self.conv(x)


class UNet(nn.Module):
    def __init__(self, n_channels, n_classes, bilinear=False):
        super(UNet, self).__init__()
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.bilinear = bilinear

        self.inc = (DoubleConv(n_channels, 64))
        self.down1 = (Down(64, 128))
        self.down2 = (Down(128, 256))
        self.down3 = (Down(256, 512))
        factor = 2 if bilinear else 1
        self.down4 = (Down(512, 1024 // factor))
        self.up1 = (Up(1024, 512 // factor, bilinear))
        self.up2 = (Up(512, 256 // factor, bilinear))
        self.up3 = (Up(256, 128 // factor, bilinear))
        self.up4 = (Up(128, 64, bilinear))
        self.outc = (OutConv(64, n_classes))

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        logits = self.outc(x)
        return logits

    def use_checkpointing(self):
        self.inc = torch.utils.checkpoint(self.inc)
        self.down1 = torch.utils.checkpoint(self.down1)
        self.down2 = torch.utils.checkpoint(self.down2)
        self.down3 = torch.utils.checkpoint(self.down3)
        self.down4 = torch.utils.checkpoint(self.down4)
        self.up1 = torch.utils.checkpoint(self.up1)
        self.up2 = torch.utils.checkpoint(self.up2)
        self.up3 = torch.utils.checkpoint(self.up3)
        self.up4 = torch.utils.checkpoint(self.up4)
        self.outc = torch.utils.checkpoint(self.outc)

import numpy as np
import torch
import torchvision
import torchvision.transforms as transforms
import os
from PIL import Image
import random
from torch.utils.data import Dataset, DataLoader
import torchvision.datasets.utils as utils


def download_data():
    url_images = "https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz"
    url_annotations = "https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz"

    data_path = './data'

    utils.download_url(url_images, data_path)
    utils.extract_archive(os.path.join(data_path, "images.tar.gz"), data_path)

    utils.download_url(url_annotations, data_path)
    utils.extract_archive(os.path.join(data_path, "annotations.tar.gz"), data_path)


#Each pixel in a mask image can take one of three values: 1, 2, or 3. 1 means that this pixel of an image belongs to the class pet, 2 - to the class background, 3 - to the class border. 
def preprocess_mask(mask):
    
    mask[np.round(mask) == 2.0] = 0.0
    mask[(np.round(mask) == 1.0) | (np.round(mask) == 3.0)] = 1.0
    return mask

class OxfordPetDataset_with_labels(Dataset):
    def __init__(self, images_filenames, images_directory, masks_directory, transform_data_1=None, transform_mask_1=None, transform_2=None):
        self.images_filenames = images_filenames
        self.images_directory = images_directory
        self.masks_directory = masks_directory

        self.transform_data_1 = transform_data_1
        self.transform_mask_1 = transform_mask_1
        self.transform_2 = transform_2

    def __len__(self):
        return len(self.images_filenames)

    def __getitem__(self, idx):
        image_filename = self.images_filenames[idx]
        image = Image.open(os.path.join(self.images_directory, image_filename))
        mask = Image.open(
            os.path.join(self.masks_directory, image_filename.replace(".jpg", ".png")),
        )
        
        if self.transform_data_1 is not None:
            image = self.transform_data_1(image)
        
        if self.transform_mask_1 is not None:
            mask = self.transform_mask_1(mask)
        
        mask = preprocess_mask(mask)

        if self.transform_2 is not None:
            mask = self.transform_2(mask)
            image = self.transform_2(image)

        mask[mask <= 0.5] = 0
        mask[mask>0.5] = 1 
        
        return image, mask



class OxfordPetDataset_with_labels_mixed(Dataset):
    def __init__(self, images_filenames, images_directory, masks_directory, unlabeled_ratio=0.8, transform_data_1=None, transform_mask_1=None, transform_2=None):
        self.images_filenames = images_filenames
        self.images_directory = images_directory
        self.masks_directory = masks_directory

        self.transform_data_1 = transform_data_1
        self.transform_mask_1 = transform_mask_1
        self.transform_2 = transform_2
        

        self.unlabeled_ratio = unlabeled_ratio

    def __len__(self):
        return len(self.images_filenames)

    def __getitem__(self, idx):
        image_filename = self.images_filenames[idx]
        image = Image.open(os.path.join(self.images_directory, image_filename))
        mask = Image.open(
            os.path.join(self.masks_directory, image_filename.replace(".jpg", ".png")),
        )
        
        if self.transform_data_1 is not None:
            image = self.transform_data_1(image)

        
        if self.transform_mask_1 is not None:
            mask = self.transform_mask_1(mask)
        
        mask = preprocess_mask(mask)

        if self.transform_2 is not None:
            image = self.transform_2(image)
            mask = self.transform_2(mask)

        mask[mask <= 0.5] = 0
        mask[mask > 0.5] = 1 
        
        # randomly change 80% of masks to -1
        if random.random() < self.unlabeled_ratio:
            mask = torch.ones((1, image.shape[1], image.shape[2])) * -1
        
        return image, mask


def readable_images(images_filenames, images_directory):
    """
    Remove the data that are not readable
    """
    correct_images_filenames = []
    
    for i in images_filenames:
        try : 
            Image.open(os.path.join(images_directory, i))
            correct_images_filenames.append(i)
        except:
            continue

    return correct_images_filenames

def are_images_all_RGB(images_filenames, images_directory):
    """
    Remove the data that do not have shape of (3,_,_)
    """
    correct_images_filenames = []
    transform = transforms.Compose([transforms.ToTensor()])
    for i in images_filenames:
        img = Image.open(os.path.join(images_directory, i))
        img = transform(img)
        if img.shape[0] == 3:
            correct_images_filenames.append(i)
    return correct_images_filenames

def get_data(nb_labeled_data, nb_unlabeled_data, percentage_validation, percentage_test, batch_size):
    """
    nb_labeled_data : number of labeled data
    nb_unlabeled_data : number of unlabeled data
    percentage_validation : percentage from the whole dataset of the validation set
    percentage_test : percentage from the whole dataset of the test set
    Output:
    dataloader for labeled training data
    dataloader for unlabeled training data
    dataloader for validation set
    dataloader for test set
    """

    assert nb_labeled_data + nb_unlabeled_data == 1

    download_data() # Comment out if you have already this downloaded

    images_directory = os.path.join("./data/images")
    masks_directory = os.path.join("./data/annotations/trimaps")

    images_filenames = list(sorted(os.listdir(images_directory)))
    print('all images are = ', len(images_filenames))
    
    correct_images_filenames = readable_images(images_filenames, images_directory)
    correct_images_filenames = are_images_all_RGB(correct_images_filenames, images_directory)

    random.shuffle(correct_images_filenames)

    
    nb_data = len(correct_images_filenames)
    
    transform_data_1 = transforms.Compose(
        [transforms.ToTensor(),
        transforms.Normalize((0, 0, 0), (1/255, 1/255, 1/255))])
    
    
    transform_mask_1 = transforms.Compose(
        [transforms.ToTensor(),
        transforms.Normalize(0, 1/255)])
    
    transform_2 = transforms.Compose(
        [transforms.Resize((128,128))]
    )

    ##train data
    index_end_train = int((1 - (percentage_validation + percentage_test)) * nb_data)
    train_images_filenames = correct_images_filenames[0:index_end_train]

    random.shuffle(train_images_filenames)

    
    

    #train mixed labeled and unlabeled data
    mixed_data_train = OxfordPetDataset_with_labels_mixed(train_images_filenames, images_directory, masks_directory, nb_unlabeled_data, transform_data_1, transform_mask_1, transform_2)
    mixed_train_loader = DataLoader(
        mixed_data_train,
        batch_size=batch_size,
        shuffle=True,
    )

    
    ##validation data
    index_start_val = index_end_train
    index_end_val = index_end_train + int(percentage_validation * nb_data)
    validation_images_filenames = correct_images_filenames[index_start_val:index_end_val]

    validation_data = OxfordPetDataset_with_labels(validation_images_filenames, images_directory, masks_directory,transform_data_1, transform_mask_1, transform_2)
    val_loader = DataLoader(
        validation_data,
        batch_size=20,
        shuffle=True,
    )

    ##test data 
    index_start_test = index_end_val
    test_images_filenames = correct_images_filenames[index_start_test :]

    test_data = OxfordPetDataset_with_labels(test_images_filenames, images_directory, masks_directory,transform_data_1,transform_mask_1, transform_2)
    test_loader = DataLoader(
        test_data,
        batch_size=20,
        shuffle=True,
    )

    return mixed_train_loader, val_loader, test_loader

import torch.nn.functional as F
import numpy as np

def softmax_mse_loss(inputs, targets):

    assert inputs.requires_grad is True and targets.requires_grad is False
    assert inputs.size() == targets.size()  # (batch_size * num_classes * H * W)
    
    inputs = F.softmax(inputs, dim=1)
    targets = F.softmax(targets, dim=1)
    
    return F.mse_loss(inputs, targets, reduction='mean')  # take the mean over the batch_size



def wt(rampup_length, current, alpha):
    if rampup_length == 0:
                return 1.0
    else:
        current = np.clip(current, 0.0, rampup_length)
        phase = 1.0 - current / rampup_length
        return float(alpha * np.exp(-5.0 * phase * phase))
    
@torch.no_grad()
def update_ema_variables(model, ema_model, alpha, global_step): 
    # Use the true average until the exponential average is more correct
    alpha = min(1 - 1 / (global_step + 1), alpha)
    for ema_param, param in zip(ema_model.parameters(), model.parameters()):
        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)

import numpy as np
import torch
import torch.nn as nn
from torch.optim import Adam

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") #Enable GPU support
mixed_train_loader, val_loader, test_loader = get_data(0.2, 0.8, 0.1, 0.1, 32)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") #Enable GPU support

sup_crit = nn.CrossEntropyLoss().to(device)

dataiter_test = iter(mixed_train_loader)
img, lab = next(dataiter_test)
img = img.to(device)

sup_crit = nn.CrossEntropyLoss(reduction='mean').to(device) 
modelS = UNet(3,2).to(device) 

lab = lab.squeeze().type(torch.LongTensor).to(device)

sup_idx = torch.tensor([(elem != -1).item() for elem in lab[:, 0, 0]]).to(device)


z = modelS(img) 
#print(z.shape, lab.shape)
Ls = sup_crit(z[sup_idx], lab[sup_idx])

def evaluate_model(loader, net, device):

    acc, no_batches = 0, 0

    # Set model to evaluation mode
    net.eval()

    with torch.no_grad():
        
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            z = net(inputs)

            probs = F.softmax(z, dim=1)
            preds = torch.argmax(probs, dim=1)

            labels = labels.squeeze().to(device)

            acc +=(preds == labels).sum() / (32*128*128)

            no_batches += 1

    return acc / no_batches

import numpy as np
import torch
import torch.nn as nn
from torch.optim import Adam


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") #Enable GPU support
#device = "cpu"

epochs, batch_size = 20, 32
ramp_up, consistency, alpha = 10, 56, 0.999


# Initialize student and teacher networks 
modelS = UNet(3,2).to(device) 
modelT = UNet(3,2).to(device)

sup_crit = nn.CrossEntropyLoss().to(device) # Supervised loss criterion
optimizer = Adam(modelS.parameters()) # Student optimizer

# Init global step
global_step = 0

print(f"\nInit all params, note we are running in {device}")

# Start training loop
for epoch in range(epochs):

        running_loss = 0

        for img, lab in mixed_train_loader:

            img = img.to(device)
            lab = lab.squeeze().type(torch.LongTensor).to(device)

            optimizer.zero_grad()

            # Forward pass for student and teacher
            z = modelS(img) 
            with torch.no_grad():
              z_bar = modelT(img)

            # Find img with label
            sup_idx = torch.tensor([(elem != -1).item() for elem in lab[:, 0, 0]]).to(device) #If batchsize is the first dim

            assert len(sup_idx) != 0

            # Calculate losses
            Ls = sup_crit(z[sup_idx], lab[sup_idx])
            Lu = softmax_mse_loss(z, z_bar).to(device)

            loss = Ls + wt(ramp_up, epoch, consistency) * Lu
            loss.backward()
            optimizer.step()    
            global_step += 1
            update_ema_variables(modelS, modelT, alpha, global_step)
            running_loss += loss.item()

        if epoch == 0:
                print('\n{:<10s}{:<10s}'.format('', 'Running loss')) 

                
        print('-'*25 +'\n' + '{:<10s}{:<10s}'.format(f"Epoch {epoch + 1}", f"{running_loss:.2f}"))